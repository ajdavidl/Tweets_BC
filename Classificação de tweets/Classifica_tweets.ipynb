{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pré-processamento\n",
    "\n",
    "## 1.1 Carrega pacotes e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pacate para a criação de data-frames\n",
    "import re #pacote para o processamento de string por meio de \"regulas expressions\"\n",
    "import nltk #pacote para o processamento de textos\n",
    "#import spacy #pacote para o processamento de textos\n",
    "import xgboost #pacote com o algoritmo extreme gradient boosting\n",
    "import numpy as np #pacote de algoritmos numéricos\n",
    "#SKLEARN é um pacote com vários algorimtos de processamento de dados e modelos de machine learning\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, tree, neural_network, neighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn.utils import resample\n",
    "from sklearn.manifold import TSNE #função usada para a redução de dimensionalidade\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import joblib\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") #omite os warnings\n",
    "\n",
    "#KERAS é um pacote para a criação de redes neurais\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "#Gensim é um pacote para processamento de words embeddings\n",
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt #pacote para visualizar dados\n",
    "import seaborn as sns # pacote para visualizar dados baseado no matplotlib\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText\n",
    "\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega base\n",
    "df = pd.read_pickle('dados\\\\df_processado.pkl')\n",
    "lista_tweets = df.tweet.to_list().copy()\n",
    "\n",
    "df = df[df['sent_manual'].fillna('nan').str.contains('N|E|S|C')]\n",
    "def corrige_label(label):\n",
    "    if label == 'S' or label == 'E':\n",
    "        return('N')\n",
    "    else:\n",
    "        return(label)\n",
    "df['sent_manual'] = df['sent_manual'].apply(corrige_label)\n",
    "\n",
    "\n",
    "corpus = df.tweet.to_list().copy()\n",
    "\n",
    "df['sent_manual'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMA PARA CAIXA BAIXA\n",
    "#REMOVE NUMEROS, PONTUACAO E URLS\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = utils.corrige_ortografia(corpus[i])\n",
    "    corpus[i]=corpus[i].lower()\n",
    "    corpus[i] = re.sub(r'http\\S+', ' ', corpus[i]) #urls (tem que ser antes dos outros)\n",
    "    corpus[i] = re.sub('\\n', ' ', corpus[i]) #newline\n",
    "    corpus[i] = re.sub('[0-9]+', ' ', corpus[i]) #números\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]',' ',corpus[i]) #pontuação\n",
    "    corpus[i] = re.sub('º','',corpus[i])\n",
    "    corpus[i] = re.sub('ª','',corpus[i])\n",
    "    corpus[i] = re.sub('@','',corpus[i])\n",
    "    corpus[i] = re.sub('#','',corpus[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = utils.altera_expressoes(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega modelo pré-treinado para processar textos em português. Desabilita duas funções que não vamos usar\n",
    "nlp = spacy.load('pt_core_news_lg', disable=['parser', 'ner'])\n",
    "\n",
    "for i in range(0,len(corpus)): # varre a lista de textos\n",
    "    doc = nlp(corpus[i]) # executa um processamento de texto\n",
    "    corpus[i]=\" \".join([token.lemma_ for token in doc]) # substitui o texto anterior por um texto contendo os lemas extraídos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = utils.corrige_lema(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Divisão dos dados em treino, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "# Divisão dos textos em um conjunto de treinamento e outro de validação\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(corpus, df.sent_manual.to_list(), \n",
    "                                                                      test_size=0.40, \n",
    "                                                                      random_state = 100, \n",
    "                                                                      stratify=df.sent_manual.to_list() )\n",
    "# Divide o conjunto de validação em validação e teste\n",
    "X_test, X_valid, y_test, y_valid = model_selection.train_test_split(X_valid, y_valid, \n",
    "                                                                      test_size=0.50, \n",
    "                                                                      random_state = 100, \n",
    "                                                                      stratify=y_valid )\n",
    "\n",
    "print(\"Treino:\",len(X_train),len(y_train))\n",
    "print(\"Validação:\",len(X_valid),len(y_valid))\n",
    "print(\"Teste:\",len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Codifica os labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia os labels\n",
    "y_train_labels = y_train.copy()\n",
    "y_valid_labels = y_valid.copy()\n",
    "y_test_labels = y_test.copy()\n",
    "\n",
    "#Tratamento dos dados de saída\n",
    "# Codificação das variveis alvo da classificação\n",
    "encoder = preprocessing.LabelEncoder() #criação do codificador\n",
    "encoder.fit(df.sent_manual)\n",
    "y_train = encoder.transform(y_train) #codificação dos dados de treinamento\n",
    "y_valid = encoder.transform(y_valid) #codificação dos dados de validação\n",
    "y_test = encoder.transform(y_test) #codificação dos dados de validação\n",
    "labels = encoder.classes_ #criação de uma lista contendo os tipos de norma (classes da classificação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Upsampling e Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = X_train\n",
    "trainDF['label'] = y_train\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = trainDF[trainDF.label==1] #'N'\n",
    "df_minority = trainDF[trainDF.label==0] #'C'\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= df_majority.shape[0] ,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_upsampled = df_upsampled.sample(df_upsampled.shape[0])\n",
    "X_train_UP = df_upsampled.text.to_list()\n",
    "y_train_UP = df_upsampled.label.to_list()\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=df_minority.shape[0],     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_downsampled = df_downsampled.sample(df_downsampled.shape[0])\n",
    "X_train_SUB = df_downsampled.text.to_list()\n",
    "y_train_SUB = df_downsampled.label.to_list()\n",
    "\n",
    "print(\"X_train:\", len(X_train))\n",
    "print(\"X_train_UP:\", len(X_train_UP))\n",
    "print(\"X_train_SUB:\", len(X_train_SUB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Remoção de stopwords\n",
    "\n",
    "Define as stopwords\n",
    "\n",
    "Faz uma cópia as listas X_train, X_train_UP, X_train_SUB, X_valid e X_test para serem usadas nas redes neurais mais complexas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords=utils.mystopwords\n",
    "\n",
    "# Faz uma cópia das listas de treinamento, validação e teste (X) para serem usadas nas redes neurais mais complexas\n",
    "X_train_NN = X_train.copy()\n",
    "X_train_UP_NN = X_train_UP.copy()\n",
    "X_train_SUB_NN = X_train_SUB.copy()\n",
    "X_valid_NN = X_valid.copy()\n",
    "X_test_NN = X_test.copy()\n",
    "\n",
    "#Remove as stopwords das listas de treino, valid e test (X) para usar nos algoritmos de ML\n",
    "for i in range(0,len(X_train)): # varre a lista de textos\n",
    "    words=X_train[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    X_train[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "for i in range(0,len(X_train_UP)): # varre a lista de textos\n",
    "    words=X_train_UP[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    X_train_UP[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "for i in range(0,len(X_train_SUB)): # varre a lista de textos\n",
    "    words=X_train_SUB[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    X_train_SUB[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "for i in range(0,len(X_valid)): # varre a lista de textos\n",
    "    words=X_valid[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    X_valid[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "for i in range(0,len(X_test)): # varre a lista de textos\n",
    "    words=X_test[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    X_test[i] = ' '.join(words_new) # concantena as palavras novamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Matrizes Termo-Documento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2000\n",
    "#Tratamento dos dados de entrada\n",
    "#DTM-FREQUÊNCIA DE PALAVRAS\n",
    "# cria um objeto contador \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                            ngram_range=(1, 1), max_features = max_tokens)\n",
    "count_vect.fit(X_train) # treina o objeto nos textos processados\n",
    "#print(count_vect.get_feature_names())\n",
    "print(\"count:\", len(count_vect.get_feature_names()),\" tokens\")\n",
    "#Transforma os documentos na matriz documento termo.\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_train_count_UP =  count_vect.transform(X_train_UP)\n",
    "X_train_count_SUB =  count_vect.transform(X_train_SUB)\n",
    "X_valid_count =  count_vect.transform(X_valid)\n",
    "X_test_count =  count_vect.transform(X_test)\n",
    "pickle.dump(count_vect.vocabulary_, open(\"dados/count-vocab\", 'wb'))\n",
    "\n",
    "#DTM-BINÁRIA\n",
    "# cria um objeto contador binário\n",
    "binary_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', binary=True,\n",
    "                             ngram_range=(1, 1), max_features = max_tokens)\n",
    "binary_vect.fit(X_train) # treina o objeto nos textos processados\n",
    "#print(binary_vect.get_feature_names())\n",
    "print(\"binary:\",len(binary_vect.get_feature_names()),\" tokens\")\n",
    "#Transforma os documentos na matriz documento termo binária.\n",
    "X_train_binary =  binary_vect.transform(X_train)\n",
    "X_train_binary_UP =  binary_vect.transform(X_train_UP)\n",
    "X_train_binary_SUB =  binary_vect.transform(X_train_SUB)\n",
    "X_valid_binary =  binary_vect.transform(X_valid)\n",
    "X_test_binary =  binary_vect.transform(X_test)\n",
    "pickle.dump(binary_vect.vocabulary_, open(\"dados/binary-vocab\", 'wb'))\n",
    "\n",
    "#DTM-TF-IDF\n",
    "# cria um objeto que calcula o TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                            ngram_range=(1, 1), max_features = max_tokens)\n",
    "tfidf_vect.fit(X_train) # treina o objeto nos textos processados\n",
    "#print(tfidf_vect.get_feature_names())\n",
    "print(\"tfidf:\",len(tfidf_vect.get_feature_names()),\" tokens\")\n",
    "#Transforma os documentos na matriz documento termo TF-IDF.\n",
    "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "X_train_tfidf_UP =  tfidf_vect.transform(X_train_UP)\n",
    "X_train_tfidf_SUB =  tfidf_vect.transform(X_train_SUB)\n",
    "X_valid_tfidf =  tfidf_vect.transform(X_valid) \n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "pickle.dump(tfidf_vect.vocabulary_, open(\"dados/tfidf-vocab\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "lista_texto_word_tokenized  = []\n",
    "for texto in lista_tweets:\n",
    "    lista_texto_word_tokenized.append(word_tokenize(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula da media e mediana para definir manualmente MAX_NB_WORDS\n",
    "x_ = []\n",
    "for l in lista_texto_word_tokenized:\n",
    "    x_.append(len(l))\n",
    "print(np.mean(np.array(x_)))\n",
    "print(np.median(np.array(x_)))\n",
    "print(np.percentile(x_,90))\n",
    "print(np.max(x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(lista_texto_word_tokenized, size=EMBEDDING_DIM, window=5, min_count=1, iter=10)\n",
    "\n",
    "model.save(\"dados\\\\tweets_gensim_fasttext.model\")\n",
    "#model = FastText.load(\"dados\\\\tweets_gensim_fasttext.model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fasttext = np.zeros((len(X_train), EMBEDDING_DIM))\n",
    "X_train_fasttext_UP = np.zeros((len(X_train_UP), EMBEDDING_DIM))\n",
    "X_train_fasttext_SUB = np.zeros((len(X_train_SUB), EMBEDDING_DIM))\n",
    "X_valid_fasttext = np.zeros((len(X_valid), EMBEDDING_DIM))\n",
    "X_test_fasttext = np.zeros((len(X_test), EMBEDDING_DIM))\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train_fasttext[i] = model[X_train[i]]\n",
    "\n",
    "for i in range(len(X_train_UP)):\n",
    "    X_train_fasttext_UP[i] = model[X_train_UP[i]]\n",
    "\n",
    "for i in range(len(X_train_SUB)):\n",
    "    X_train_fasttext_SUB[i] = model[X_train_SUB[i]]\n",
    "\n",
    "for i in range(len(X_valid)):\n",
    "    X_valid_fasttext[i] = model[X_valid[i]]\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test_fasttext[i] = model[X_test[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(corpus)\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train_NN), maxlen=MAX_NB_WORDS)\n",
    "X_train_seq_UP = sequence.pad_sequences(token.texts_to_sequences(X_train_UP_NN), maxlen=MAX_NB_WORDS)\n",
    "X_train_seq_SUB = sequence.pad_sequences(token.texts_to_sequences(X_train_SUB_NN), maxlen=MAX_NB_WORDS)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid_NN), maxlen=MAX_NB_WORDS)\n",
    "X_test_seq = sequence.pad_sequences(token.texts_to_sequences(X_test_NN), maxlen=MAX_NB_WORDS)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = model[word] #embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Definição de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função que treina os algoritmos de classificação\n",
    "def train_model(classifier, train_x, train_y, test_x, test_y, save = False, nome_arquivo = None, nome_modelo = None, parameters = None, smote=False):\n",
    "    \"\"\"\n",
    "    #Parâmetros\n",
    "    #classifier: algoritmo de classificação\n",
    "    #train_x: dados de treinamento de entrada (X)\n",
    "    #train_y: dados de treinamento de saída (Y)\n",
    "    #test_x: dados de teste de entrada (X)\n",
    "    #test_y: dados de teste de saída (Y)\n",
    "    #save: salva o modelo treinado. Default = False\n",
    "    #nome_arquivo: nome do arquivo para salvar o modelo\n",
    "    #nome_modelo: nome do modelo para salvar no arquivo \"resultados-classificacao.csv\"\n",
    "    #parameters: parâmetros do classificador para serem testados pelo GridSearch\n",
    "    #smote: bool para aplicar SMOTE ou não\n",
    "    \"\"\"\n",
    "    \n",
    "    if smote:\n",
    "        # APLICA A TÉCNICA DO SMOTE\n",
    "        oversample = SMOTE(random_state=100, n_jobs=-1)\n",
    "        train_x, train_y = oversample.fit_resample(train_x, train_y)\n",
    "    \n",
    "    if (__name__ == \"__main__\") & (parameters != None) :\n",
    "        # multiprocessing requires the fork to happen in a __main__ protected\n",
    "        # block\n",
    "\n",
    "        # find the best parameters for both the feature extraction and the\n",
    "        # classifier\n",
    "        grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=0,cv=5)\n",
    "        t0 = time()\n",
    "        grid_search.fit(train_x, train_y)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        predictions= grid_search.best_estimator_.predict(test_x)\n",
    "        classifier = grid_search.best_estimator_\n",
    "        \n",
    "    else:\n",
    "        # treina o classificador\n",
    "        classifier.fit(train_x, train_y)\n",
    "        # realiza uma previsão da classificação com base nos dados de teste\n",
    "        predictions = classifier.predict(test_x)\n",
    "    \n",
    "\n",
    "    #calcula a matriz de confusão\n",
    "    confusionMatrix(predictions, test_y)\n",
    "    print(\"\\n\") #pula uma linha\n",
    "    #cria um relatório com base nas previsões realizdas\n",
    "    classificationReport(predictions, test_y)\n",
    "    \n",
    "    #calcula o kapppa\n",
    "    kappa = metrics.cohen_kappa_score(test_y, predictions)\n",
    "    print(\"Kappa score: {:.3f}\\n\".format(kappa))\n",
    "    acc = metrics.accuracy_score(test_y, predictions)\n",
    "    print(\"Accuracy score: {:.3f}\\n\".format(acc))\n",
    "    f1 = metrics.f1_score(test_y, predictions, average='weighted')\n",
    "    print(\"f1 weighted score: {:.3f}\\n\".format(f1))\n",
    "    acc_bal = metrics.balanced_accuracy_score(test_y, predictions)\n",
    "    print(\"Balanced Accuracy score: {:.3f}\\n\".format(acc_bal))\n",
    "    roc = metrics.roc_auc_score(test_y, predictions)\n",
    "    print(\"Area under the ROC curve: {:.3f}\\n\".format(roc))\n",
    "    rec = metrics.recall_score(test_y, predictions, pos_label = 0, average='binary')\n",
    "    print(\"Recall classe C: {:.3f}\\n\".format(rec))\n",
    "    prec = metrics.precision_score(test_y, predictions, pos_label = 0, average='binary')\n",
    "    print(\"Precision classe C: {:.3f}\\n\".format(prec))\n",
    "\n",
    "    \n",
    "    #salva se for o caso\n",
    "    if save:\n",
    "        if not nome_arquivo:\n",
    "            nome_arquivo = type(classifier).__name__\n",
    "        joblib.dump(classifier, \"dados/class-\"+nome_arquivo)\n",
    "\n",
    "    if nome_modelo:\n",
    "        dateTimeObj = datetime.now()\n",
    "        with open(\"Classificação de tweets\\\\resultados-classificacao.csv\", \"a\") as myfile:\n",
    "            myfile.write(nome_modelo+\",\"+str(test_x.shape[0])+\",\"+str(acc)+\",\"+str(kappa)+\",\"+str(f1)+\",\"+str(acc_bal)+\",\"+str(roc)+\",\"+str(rec)+\",\"+str(prec)+\",\"+dateTimeObj.strftime(\"%Y-%m-%d\")+\"\\n\")\n",
    "    # retorna a acurácia do modelo        \n",
    "    return  classifier\n",
    "\n",
    "# função que calcula a matriz de confusão\n",
    "def confusionMatrix(predictions, real):\n",
    "    #faz um processamento dos dados para uma melhor impressão\n",
    "    X = np.array( metrics.confusion_matrix(y_true=real,y_pred=predictions))\n",
    "    X = pd.DataFrame(X,index = labels, columns = labels)\n",
    "    print(X)\n",
    "    return\n",
    "\n",
    "# função que cria um relatório com base nas previsões realizadas pelo modelo\n",
    "def classificationReport(predictions, real):\n",
    "    print(metrics.classification_report(y_true=real,y_pred=predictions, target_names=labels))    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#VISUALIZANDO OS DADOS\n",
    "\n",
    "#Redução da dimensionalidade das matrizes DTm usando o algoritmo T-SNE\n",
    "#X_train_count_embedded = TSNE(n_components=2).fit_transform(X_train_count.toarray())\n",
    "#X_train_binary_embedded = TSNE(n_components=2).fit_transform(X_train_binary.toarray())\n",
    "#X_train_tfidf_embedded = TSNE(n_components=2).fit_transform(X_train_tfidf.toarray())\n",
    "X_train_fasttext_embedded = TSNE(n_components=2).fit_transform(X_train_fasttext)\n",
    "\n",
    "#Plota os gráficos\n",
    "#sns.scatterplot(x=X_train_count_embedded[:,0], y=X_train_count_embedded[:,1], hue=train_y_labels)\n",
    "#plt.title('DTM - Frequência de Palavras')\n",
    "#plt.xlabel('x')\n",
    "#plt.ylabel('y')\n",
    "#plt.show()\n",
    "\n",
    "#sns.scatterplot(x=X_train_binary_embedded[:,0], y=X_train_binary_embedded[:,1], hue=train_y_labels)\n",
    "#plt.title('DTM - Binária')\n",
    "#plt.xlabel('x')\n",
    "#plt.ylabel('y')\n",
    "#plt.show()\n",
    "\n",
    "#sns.scatterplot(x=X_train_tfidf_embedded[:,0], y=X_train_tfidf_embedded[:,1], hue=train_y_labels)\n",
    "#plt.title('DTM - TFIDF')\n",
    "#plt.xlabel('x')\n",
    "#plt.ylabel('y')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "sns_plot = sns.scatterplot(x=X_train_fasttext_embedded[:,0], y=X_train_fasttext_embedded[:,1], palette=['blue','red'],\n",
    "                hue=pd.Series(y_train).apply(lambda x:'C' if x==0 else 'N').values)\n",
    "plt.title('TSNE - FASTTEXT')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig(\"imagens\\\\DTM-fasttext.png\", dpi=900)\n",
    "plt.show()\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Treinamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NAIVE BAYES\n",
    "nome = \"MULTINOMIAL NAIVE BAYES\"\n",
    "nome2 = \"MultinomialNB\"\n",
    "modelosNB = [naive_bayes.MultinomialNB() for i in range(16)]\n",
    "parameters_ = {'alpha': (0.0, 0.5, 1.0)}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosNB[0] = train_model(modelosNB[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosNB[1] = train_model(modelosNB[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosNB[2] = train_model(modelosNB[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosNB[3] = train_model(modelosNB[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosNB[4] = train_model(modelosNB[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosNB[5] = train_model(modelosNB[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosNB[6] = train_model(modelosNB[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosNB[7] = train_model(modelosNB[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosNB[8] = train_model(modelosNB[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosNB[9] = train_model(modelosNB[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosNB[10] = train_model(modelosNB[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosNB[11] = train_model(modelosNB[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "#MULTINOMIAL NB NÃO ACEITA VALORES NEGATIVOS\n",
    "# FastText Vectors\n",
    "#print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "#modelosNB[12] = train_model(modelosNB[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "#print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "#modelosNB[13] = train_model(modelosNB[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "#print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "#modelosNB[14] = train_model(modelosNB[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "#print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "#modelosNB[15] = train_model(modelosNB[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NAIVE BAYES\n",
    "nome = \"GAUSSIAN NAIVE BAYES\"\n",
    "nome2 = \"GaussianNB\"\n",
    "modelosGNB = [naive_bayes.GaussianNB() for i in range(16)]\n",
    "parameters_ = None\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosGNB[0] = train_model(modelosGNB[0], X_train_count.toarray(), y_train, X_test_count.toarray(), y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosGNB[1] = train_model(modelosGNB[1], X_train_count_UP.toarray(), y_train_UP, X_test_count.toarray(), y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosGNB[2] = train_model(modelosGNB[2], X_train_count_SUB.toarray(), y_train_SUB, X_test_count.toarray(), y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosGNB[3] = train_model(modelosGNB[3], X_train_count.toarray(), y_train, X_test_count.toarray(), y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosGNB[4] = train_model(modelosGNB[4], X_train_binary.toarray(), y_train, X_test_binary.toarray(), y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosGNB[5] = train_model(modelosGNB[5], X_train_binary_UP.toarray(), y_train_UP, X_test_binary.toarray(), y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosGNB[6] = train_model(modelosGNB[6], X_train_binary_SUB.toarray(), y_train_SUB, X_test_binary.toarray(), y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosGNB[7] = train_model(modelosGNB[7], X_train_binary.toarray(), y_train, X_test_binary.toarray(), y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosGNB[8] = train_model(modelosGNB[8], X_train_tfidf.toarray(), y_train, X_test_tfidf.toarray(), y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosGNB[9] = train_model(modelosGNB[9], X_train_tfidf_UP.toarray(), y_train_UP, X_test_tfidf.toarray(), y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosGNB[10] = train_model(modelosGNB[10], X_train_tfidf_SUB.toarray(), y_train_SUB, X_test_tfidf.toarray(), y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosGNB[11] = train_model(modelosGNB[11], X_train_tfidf.toarray(), y_train, X_test_tfidf.toarray(), y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosGNB[12] = train_model(modelosGNB[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosGNB[13] = train_model(modelosGNB[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosGNB[14] = train_model(modelosGNB[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosGNB[15] = train_model(modelosGNB[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "nome = \"KNN\"\n",
    "nome2 = \"KNeighbors\"\n",
    "modelosKN = [neighbors.KNeighborsClassifier() for i in range(16)]\n",
    "parameters_ = {'n_neighbors' : (1, 3, 5, 7, 9),\n",
    "               'weights' : ('uniform', 'distance'),\n",
    "               'p' : (1, 2)}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosKN[0] = train_model(modelosKN[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosKN[1] = train_model(modelosKN[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosKN[2] = train_model(modelosKN[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosKN[3] = train_model(modelosKN[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosKN[4] = train_model(modelosKN[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosKN[5] = train_model(modelosKN[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosKN[6] = train_model(modelosKN[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosKN[7] = train_model(modelosKN[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosKN[8] = train_model(modelosKN[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosKN[9] = train_model(modelosKN[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosKN[10] = train_model(modelosKN[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosKN[11] = train_model(modelosKN[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosKN[12] = train_model(modelosKN[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosKN[13] = train_model(modelosKN[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosKN[14] = train_model(modelosKN[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosKN[15] = train_model(modelosKN[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent (SGD)\n",
    "nome = \"STOCHASTIC GRADIENT DESCENT\"\n",
    "nome2 = \"SGDClassifier\"\n",
    "modelosSGD = [linear_model.SGDClassifier() for i in range(16)]\n",
    "parameters_ = {'penalty': ('L1', 'l2', 'elasticnet')}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosSGD[0] = train_model(modelosSGD[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosSGD[1] = train_model(modelosSGD[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosSGD[2] = train_model(modelosSGD[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosSGD[3] = train_model(modelosSGD[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosSGD[4] = train_model(modelosSGD[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosSGD[5] = train_model(modelosSGD[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosSGD[6] = train_model(modelosSGD[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosSGD[7] = train_model(modelosSGD[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosSGD[8] = train_model(modelosSGD[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosSGD[9] = train_model(modelosSGD[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosSGD[10] = train_model(modelosSGD[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosSGD[11] = train_model(modelosSGD[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosSGD[12] = train_model(modelosSGD[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosSGD[13] = train_model(modelosSGD[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosSGD[14] = train_model(modelosSGD[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosSGD[15] = train_model(modelosSGD[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#REGRESSÃO LOGÍSTICA\n",
    "nome = \"REGRESSÃO LOGÍSTICA\"\n",
    "nome2 = \"LogisticRegression\"\n",
    "modelosRL = [linear_model.LogisticRegression() for i in range(16)]\n",
    "parameters_ = {'penalty':('l1', 'l2', 'elasticnet'),\n",
    "               'C':(0.5, 1.0),\n",
    "               'class_weight' : ('balanced',None)}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "#modelosRL[0] = train_model(modelosRL[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "modelosRL[0] = train_model(modelosRL[0], X_train_count, y_train, X_test_count, y_test,\n",
    "                           save = True, nome_arquivo=nome2+\"-count\",\n",
    "                           nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosRL[1] = train_model(modelosRL[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosRL[2] = train_model(modelosRL[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosRL[3] = train_model(modelosRL[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosRL[4] = train_model(modelosRL[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosRL[5] = train_model(modelosRL[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosRL[6] = train_model(modelosRL[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosRL[7] = train_model(modelosRL[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosRL[8] = train_model(modelosRL[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosRL[9] = train_model(modelosRL[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosRL[10] = train_model(modelosRL[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosRL[11] = train_model(modelosRL[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosRL[12] = train_model(modelosRL[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosRL[13] = train_model(modelosRL[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosRL[14] = train_model(modelosRL[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosRL[15] = train_model(modelosRL[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "nome = \"SVM\"\n",
    "nome2 = \"SVM\"\n",
    "modelosSVM = [svm.SVC() for i in range(16)]\n",
    "parameters_ = {'C': (0.0, 0.5, 1.0),\n",
    "               'kernel':('linear','poly','rbf','sigmoid'),\n",
    "               'class_weight' : ('balanced',None)}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "#modelosSVM[0] = train_model(modelosSVM[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "modelosSVM[0] = train_model(modelosSVM[0], X_train_count, y_train, X_test_count, y_test, \n",
    "                            save = True, nome_arquivo=nome2+\"-count\",\n",
    "                            nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosSVM[1] = train_model(modelosSVM[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosSVM[2] = train_model(modelosSVM[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosSVM[3] = train_model(modelosSVM[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosSVM[4] = train_model(modelosSVM[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosSVM[5] = train_model(modelosSVM[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosSVM[6] = train_model(modelosSVM[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosSVM[7] = train_model(modelosSVM[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "#modelosSVM[8] = train_model(modelosSVM[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "modelosSVM[8] = train_model(modelosSVM[8], X_train_tfidf, y_train, X_test_tfidf, y_test, \n",
    "                            save = True, nome_arquivo=nome2+\"-tfidf\",\n",
    "                            nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosSVM[9] = train_model(modelosSVM[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosSVM[10] = train_model(modelosSVM[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosSVM[11] = train_model(modelosSVM[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosSVM[12] = train_model(modelosSVM[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosSVM[13] = train_model(modelosSVM[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosSVM[14] = train_model(modelosSVM[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosSVM[15] = train_model(modelosSVM[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DECISION TREE\n",
    "nome = \"DECISION TREE\"\n",
    "nome2 = \"DecisionTree\"\n",
    "modelosAD = [tree.DecisionTreeClassifier() for i in range(16)]\n",
    "parameters_ = {'criterion': ('gini', 'entropy'),\n",
    "               'splitter':('best','random'),\n",
    "               'max_depth':(10, 20, 40, 50, None),\n",
    "               'class_weight' : ('balanced',None)\n",
    "               }\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "#modelosAD[0] = train_model(modelosAD[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "modelosAD[0] = train_model(modelosAD[0], X_train_count, y_train, X_test_count, y_test, \n",
    "                           save = True, nome_arquivo = nome2+\"-count\",\n",
    "                           nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosAD[1] = train_model(modelosAD[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosAD[2] = train_model(modelosAD[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosAD[3] = train_model(modelosAD[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosAD[4] = train_model(modelosAD[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosAD[5] = train_model(modelosAD[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosAD[6] = train_model(modelosAD[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosAD[7] = train_model(modelosAD[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosAD[8] = train_model(modelosAD[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosAD[9] = train_model(modelosAD[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosAD[10] = train_model(modelosAD[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosAD[11] = train_model(modelosAD[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosAD[12] = train_model(modelosAD[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosAD[13] = train_model(modelosAD[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosAD[14] = train_model(modelosAD[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosAD[15] = train_model(modelosAD[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "nome = \"RANDOM FOREST\"\n",
    "nome2 = \"RandomForest\"\n",
    "modelosRF = [ensemble.RandomForestClassifier(random_state=100) for i in range(16)]\n",
    "parameters_ = {'n_estimators' : (50, 75, 100),\n",
    "               'criterion': ('gini', 'entropy'),\n",
    "               'max_depth':(20, 40, 50, None),\n",
    "               'class_weight' : ('balanced','balanced_subsample',None)\n",
    "               }\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "#modelosRF[0] = train_model(modelosRF[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "modelosRF[0] = train_model(modelosRF[0], X_train_count, y_train, X_test_count, y_test, \n",
    "                           save = True, nome_arquivo=nome2+\"-count\",\n",
    "                           nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosRF[1] = train_model(modelosRF[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosRF[2] = train_model(modelosRF[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosRF[3] = train_model(modelosRF[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosRF[4] = train_model(modelosRF[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosRF[5] = train_model(modelosRF[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosRF[6] = train_model(modelosRF[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosRF[7] = train_model(modelosRF[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosRF[8] = train_model(modelosRF[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosRF[9] = train_model(modelosRF[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosRF[10] = train_model(modelosRF[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosRF[11] = train_model(modelosRF[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosRF[12] = train_model(modelosRF[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosRF[13] = train_model(modelosRF[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosRF[14] = train_model(modelosRF[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosRF[15] = train_model(modelosRF[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EXTREME GRADIENT BOOSTING\n",
    "nome = \"EXTREME GRADIENT BOOSTING\"\n",
    "nome2 = \"xgboost.XGBC\"\n",
    "modelosXGB = [xgboost.XGBClassifier(seed=100, random_state=100) for i in range(16)]\n",
    "parameters_ = None#{}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosXGB[0] = train_model(modelosXGB[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosXGB[1] = train_model(modelosXGB[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosXGB[2] = train_model(modelosXGB[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosXGB[3] = train_model(modelosXGB[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosXGB[4] = train_model(modelosXGB[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosXGB[5] = train_model(modelosXGB[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosXGB[6] = train_model(modelosXGB[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosXGB[7] = train_model(modelosXGB[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosXGB[8] = train_model(modelosXGB[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosXGB[9] = train_model(modelosXGB[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosXGB[10] = train_model(modelosXGB[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosXGB[11] = train_model(modelosXGB[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosXGB[12] = train_model(modelosXGB[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosXGB[13] = train_model(modelosXGB[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosXGB[14] = train_model(modelosXGB[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosXGB[15] = train_model(modelosXGB[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PERCEPTRON\n",
    "nome = \"PERCEPTRON\"\n",
    "nome2 = \"Perceptron\"\n",
    "modelosP = [linear_model.Perceptron() for i in range(16)]\n",
    "parameters_ = {'penalty':('l1', 'l2', 'elasticnet'),\n",
    "               'class_weight' : ('balanced','weight',None)}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosP[0] = train_model(modelosP[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosP[1] = train_model(modelosP[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosP[2] = train_model(modelosP[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosP[3] = train_model(modelosP[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosP[4] = train_model(modelosP[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosP[5] = train_model(modelosP[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosP[6] = train_model(modelosP[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosP[7] = train_model(modelosP[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosP[8] = train_model(modelosP[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosP[9] = train_model(modelosP[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosP[10] = train_model(modelosP[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosP[11] = train_model(modelosP[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosP[12] = train_model(modelosP[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosP[13] = train_model(modelosP[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosP[14] = train_model(modelosP[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosP[15] = train_model(modelosP[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MULTI-LAYER PERCEPTRON\n",
    "nome = \"MULTI-LAYER PERCEPTRON\"\n",
    "nome2 = \"MLP\"\n",
    "modelosMLP = [neural_network.MLPClassifier(random_state=100) for i in range(16)]\n",
    "parameters_ = {'activation':('relu', 'logistic')}\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosMLP[0] = train_model(modelosMLP[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosMLP[1] = train_model(modelosMLP[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosMLP[2] = train_model(modelosMLP[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosMLP[3] = train_model(modelosMLP[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosMLP[4] = train_model(modelosMLP[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosMLP[5] = train_model(modelosMLP[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosMLP[6] = train_model(modelosMLP[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosMLP[7] = train_model(modelosMLP[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosMLP[8] = train_model(modelosMLP[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosMLP[9] = train_model(modelosMLP[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosMLP[10] = train_model(modelosMLP[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosMLP[11] = train_model(modelosMLP[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosMLP[12] = train_model(modelosMLP[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosMLP[13] = train_model(modelosMLP[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosMLP[14] = train_model(modelosMLP[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosMLP[15] = train_model(modelosMLP[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VOTING CLASSIFIER\n",
    "nome = \"VOTING CLASSIFIER\"\n",
    "nome2 = \"Voting\"\n",
    "modelosVC = []\n",
    "parameters_ = None\n",
    "    \n",
    "\n",
    "for i in range(16):\n",
    "    eclf = ensemble.VotingClassifier(\n",
    "    estimators=[('log_reg', modelosRL[i]), \n",
    "                ('SVM', modelosSVM[i]), \n",
    "                ('RF', modelosRF[i]), \n",
    "                ('xgb', modelosXGB[i]),\n",
    "                ('MLP', modelosMLP[i])\n",
    "               ],\n",
    "    voting = 'hard')\n",
    "    modelosVC.append(eclf)\n",
    "\n",
    "# Count Vectors \n",
    "print (\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "modelosVC[0] = train_model(modelosVC[0], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "modelosVC[1] = train_model(modelosVC[1], X_train_count_UP, y_train_UP, X_test_count, y_test, nome_modelo = nome2+\"-count-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "modelosVC[2] = train_model(modelosVC[2], X_train_count_SUB, y_train_SUB, X_test_count, y_test, nome_modelo = nome2+\"-count-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - COUNT VECTORS Smote\")\n",
    "modelosVC[3] = train_model(modelosVC[3], X_train_count, y_train, X_test_count, y_test, nome_modelo = nome2+\"-count-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "\n",
    "# Binary Vectors\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "modelosVC[4] = train_model(modelosVC[4], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "modelosVC[5] = train_model(modelosVC[5], X_train_binary_UP, y_train_UP, X_test_binary, y_test, nome_modelo = nome2+\"-binary-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Dowsampling\")\n",
    "modelosVC[6] = train_model(modelosVC[6], X_train_binary_SUB, y_train_SUB, X_test_binary, y_test, nome_modelo = nome2+\"-binary-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - BINARY VECTORS Smote\")\n",
    "modelosVC[7] = train_model(modelosVC[7], X_train_binary, y_train, X_test_binary, y_test, nome_modelo = nome2+\"-binary-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# TF IDF Vectors\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "modelosVC[8] = train_model(modelosVC[8], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "modelosVC[9] = train_model(modelosVC[9], X_train_tfidf_UP, y_train_UP, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "modelosVC[10] = train_model(modelosVC[10], X_train_tfidf_SUB, y_train_SUB, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - TF-IDF VECTORS Smote\")\n",
    "modelosVC[11] = train_model(modelosVC[11], X_train_tfidf, y_train, X_test_tfidf, y_test, nome_modelo = nome2+\"-tfidf-Smote\", parameters = parameters_, smote=True)\n",
    "\n",
    "\n",
    "# FastText Vectors\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "modelosVC[12] = train_model(modelosVC[12], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "modelosVC[13] = train_model(modelosVC[13], X_train_fasttext_UP, y_train_UP, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-UP\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "modelosVC[14] = train_model(modelosVC[14], X_train_fasttext_SUB, y_train_SUB, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-SUB\", parameters = parameters_)\n",
    "\n",
    "print (\"\\n\",nome,\" - FASTTEXT VECTORS Smote\")\n",
    "modelosVC[15] = train_model(modelosVC[15], X_train_fasttext, y_train, X_test_fasttext, y_test, nome_modelo = nome2+\"-fasttext-Smote\", parameters = parameters_, smote=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Redes Neurais \n",
    "\n",
    "## Definição de Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para criar uma rede neural simples (feed foward)\n",
    "def create_model_architecture(input_size):\n",
    "    # cria a camada de entrada\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # cria a camada interna com 10000 nós\n",
    "    hidden_layer = layers.Dense(10000, activation=\"relu\")(input_layer)\n",
    "    hidden_layer = layers.Dropout(0.25)(hidden_layer)\n",
    "    # cria a camada de saída\n",
    "    output_layer = layers.Dense(2, activation=\"sigmoid\")(hidden_layer)\n",
    "    #cria o modelo da rede neural\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    #compila o modelo atribuindo alguns parâmetros\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #retorna o modelo classificador\n",
    "    return classifier \n",
    "\n",
    "# função para treinar todas as redes neurais \n",
    "def train_model_neural_network(classifier, train_x, train_y, valid_x, valid_y, test_x, test_y, save = False, nome_arquivo = None, epochs=1, nome_modelo = None):\n",
    "    \n",
    "    #cria dados dummies de treino, teste e validação\n",
    "    train_y_dummies = pd.get_dummies(train_y).values\n",
    "    valid_y_dummies = pd.get_dummies(valid_y).values\n",
    "    test_y_dummies = pd.get_dummies(test_y).values\n",
    "    #Agrupa dados de validação\n",
    "    valid = (valid_x, valid_y_dummies)\n",
    "    \n",
    "    checkpoint_filepath = 'tmp\\\\weights.hdf5'\n",
    "    cbks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-4 less\"\n",
    "        min_delta=1e-4,\n",
    "        # \"no longer improving\" being further defined as \"for at least 3 epochs\"\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        ),\n",
    "    callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True) \n",
    "    ]\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    #batch_size = 20\n",
    "    #treina a rede neural\n",
    "    classifier.fit(train_x, train_y_dummies, epochs=epochs, validation_data=valid, callbacks=cbks)\n",
    "    classifier.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    # faz a previsão da saída do modelo com base nos dados de teste\n",
    "    predictions = classifier.predict(test_x)\n",
    "    print(\"\\n\")\n",
    "    confusionMatrix_neural_networks(predictions, test_y_dummies) # imprime a matriz de confusão\n",
    "    print(\"\\n\")\n",
    "    classificationReport_neural_networks(predictions, test_y_dummies) # imprime o relatório da validação dos dados\n",
    "    \n",
    "    #recupera os labels\n",
    "    test_labels=labels[test_y_dummies.argmax(1)]\n",
    "    predictions_labels=labels[predictions.argmax(1)]\n",
    "    \n",
    "    #retorna o perda (loss) e a acurácia do modelo\n",
    "    accuracy = classifier.evaluate(valid_x, valid_y_dummies)\n",
    "    print('Valid set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accuracy[0], accuracy[1]))\n",
    "    print(\"Test set:\")\n",
    "    #calcula o kapppa\n",
    "    kappa = metrics.cohen_kappa_score(test_labels, predictions_labels)\n",
    "    print(\"Kappa score: {:.3f}\\n\".format(kappa))\n",
    "    acc = metrics.accuracy_score(test_labels, predictions_labels)\n",
    "    print(\"Accuracy score: {:.3f}\\n\".format(acc))\n",
    "    f1 = metrics.f1_score(test_labels, predictions_labels, average='weighted')\n",
    "    print(\"f1 macro score: {:.3f}\\n\".format(f1))\n",
    "    acc_bal = metrics.balanced_accuracy_score(test_labels, predictions_labels)\n",
    "    print(\"Balanced Accuracy score: {:.3f}\\n\".format(acc_bal))\n",
    "    roc = metrics.roc_auc_score(test_y, predictions.argmax(1))\n",
    "    print(\"Area under the ROC curve: {:.3f}\\n\".format(roc))\n",
    "    rec = metrics.recall_score(test_labels, predictions_labels, pos_label = 'C', average='binary')\n",
    "    print(\"Recall classe C: {:.3f}\\n\".format(rec))\n",
    "    prec = metrics.precision_score(test_labels, predictions_labels, pos_label = 'C', average='binary')\n",
    "    print(\"Precision classe C: {:.3f}\\n\".format(prec))\n",
    "    \n",
    "    #salva se for o caso\n",
    "    if save:\n",
    "        if not nome_arquivo:\n",
    "            nome_arquivo = type(classifier).__name__\n",
    "        classifier.save(nome_arquivo)\n",
    "    \n",
    "    if nome_modelo:\n",
    "        dateTimeObj = datetime.now()\n",
    "        with open(\"Classificação de tweets\\\\resultados-classificacao.csv\", \"a\") as myfile:\n",
    "            myfile.write(nome_modelo+\",\"+str(test_x.shape[0])+\",\"+str(acc)+\",\"+str(kappa)+\",\"+str(f1)+\",\"+str(acc_bal)+\",\"+str(roc)+\",\"+str(rec)+\",\"+str(prec)+\",\"+dateTimeObj.strftime(\"%Y-%m-%d\")+\"\\n\")\n",
    "            \n",
    "    \n",
    "    return \n",
    "# função para a impressão da matriz de confusão de redes neurais\n",
    "def confusionMatrix_neural_networks(predictions, y_dummies):\n",
    "    X = np.array( metrics.confusion_matrix(y_true=labels[y_dummies.argmax(1)],y_pred=labels[predictions.argmax(1)]))\n",
    "    X = pd.DataFrame(X,index = labels, columns = labels)\n",
    "    print(X)\n",
    "    return\n",
    "# função para a impressão do relatório de classificação de redes neurais\n",
    "def classificationReport_neural_networks(predictions, y_dummies):\n",
    "    print(metrics.classification_report(y_true=labels[y_dummies.argmax(1)],y_pred=labels[predictions.argmax(1)], target_names=labels))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Rede neural simples \n",
    "\n",
    "Esse treino utiliza as matrizes Termo-Documento como dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#REDE NEURAL SIMPLES\n",
    "nome = \"REDE NEURAL SIMPLES\"\n",
    "nome2 = \"NN\"\n",
    "\n",
    "print(\"\\n\",nome,\" - COUNT VECTORS\")\n",
    "classifier = create_model_architecture(X_train_count.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-countVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_count.todense(), y_train, X_valid_count.todense(), y_valid, X_test_count.todense(), y_test, epochs=50,nome_modelo = nome2+\"-count\")\n",
    "\n",
    "print(\"\\n\",nome,\" - COUNT VECTORS Upsampling\")\n",
    "classifier = create_model_architecture(X_train_count_UP.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-countVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_count_UP.todense(), y_train_UP, X_valid_count.todense(), y_valid, X_test_count.todense(), y_test, epochs=50,nome_modelo = nome2+\"-count-UP\")\n",
    "\n",
    "print(\"\\n\",nome,\" - COUNT VECTORS Downsampling\")\n",
    "classifier = create_model_architecture(X_train_count_SUB.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-countVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_count_SUB.todense(), y_train_SUB, X_valid_count.todense(), y_valid, X_test_count.todense(), y_test, epochs=50,nome_modelo = nome2+\"-count-SUB\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\",nome,\" - BINARY VECTORS\")\n",
    "classifier = create_model_architecture(X_train_binary.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-binaryVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_binary.todense(), y_train, X_valid_binary.todense(), y_valid, X_test_binary.todense(), y_test, epochs=50,nome_modelo = nome2+\"-binary\")\n",
    "\n",
    "print(\"\\n\",nome,\" - BINARY VECTORS Upsampling\")\n",
    "classifier = create_model_architecture(X_train_binary_UP.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-binaryVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_binary_UP.todense(), y_train_UP, X_valid_binary.todense(), y_valid, X_test_binary.todense(), y_test, epochs=50,nome_modelo = nome2+\"-binary-UP\")\n",
    "\n",
    "print(\"\\n\",nome,\" - BINARY VECTORS Downsampling\")\n",
    "classifier = create_model_architecture(X_train_binary_SUB.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-binaryVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_binary_SUB.todense(), y_train_SUB, X_valid_binary.todense(), y_valid, X_test_binary.todense(), y_test, epochs=50,nome_modelo = nome2+\"-binary-SUB\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\",nome,\" - TF-IDF VECTORS\")\n",
    "classifier = create_model_architecture(X_train_tfidf.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-tfidfVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_tfidf.todense(), y_train, X_valid_tfidf.todense(), y_valid, X_test_tfidf.todense(), y_test, epochs=50,nome_modelo = nome2+\"-tfidf\")\n",
    "\n",
    "print(\"\\n\",nome,\" - TF-IDF VECTORS Upsampling\")\n",
    "classifier = create_model_architecture(X_train_tfidf_UP.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-tfidfVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_tfidf_UP.todense(), y_train_UP, X_valid_tfidf.todense(), y_valid, X_test_tfidf.todense(), y_test, epochs=50,nome_modelo = nome2+\"-tfidf-UP\")\n",
    "\n",
    "print(\"\\n\",nome,\" - TF-IDF VECTORS Downsampling\")\n",
    "classifier = create_model_architecture(X_train_tfidf_SUB.todense().shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-tfidfVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_tfidf_SUB.todense(), y_train_SUB, X_valid_tfidf.todense(), y_valid, X_test_tfidf.todense(), y_test, epochs=50,nome_modelo = nome2+\"-tfidf-SUB\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\",nome,\" - FASTTEXT VECTORS\")\n",
    "classifier = create_model_architecture(X_train_fasttext.shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-fasttextVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_fasttext, y_train, X_valid_fasttext, y_valid, X_test_fasttext, y_test, epochs=50,nome_modelo = nome2+\"-fasttext\")\n",
    "\n",
    "print(\"\\n\",nome,\" - FASTTEXT VECTORS Upsampling\")\n",
    "classifier = create_model_architecture(X_train_fasttext_UP.shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-fasttextVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_fasttext_UP, y_train_UP, X_valid_fasttext, y_valid, X_test_fasttext, y_test, epochs=50,nome_modelo = nome2+\"-fasttext-UP\")\n",
    "\n",
    "print(\"\\n\",nome,\" - FASTTEXT VECTORS Downsampling\")\n",
    "classifier = create_model_architecture(X_train_fasttext_SUB.shape[1])\n",
    "#classifier = models.load_model('SimpleNeuralNetwork-fasttextVectors.h5')\n",
    "train_model_neural_network(classifier, X_train_fasttext_SUB, y_train_SUB, X_valid_fasttext, y_valid, X_test_fasttext, y_test, epochs=50,nome_modelo = nome2+\"-fasttext-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rede Neural Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDE NEURAL CONVOLUCIONAL\n",
    "\n",
    "\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nome = \"REDE NEURAL CONVOLUCIONAL\"\n",
    "nome2 = \"CNN\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_cnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_cnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_cnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Rede Neural Recorrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDE NEURAL RECORRENTE - LSTM\n",
    "\n",
    "def create_rnn():\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the RNN Layer\n",
    "    rnn_layer = layers.SimpleRNN(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(rnn_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nome = \"REDE NEURAL RECORRENTE\"\n",
    "nome2 = \"RNN\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_rnn()\n",
    "#classifier = models.load_model('RNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_rnn()\n",
    "#classifier = models.load_model('RNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_rnn()\n",
    "#classifier = models.load_model('RNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Rede Neural Recorrente - LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDE NEURAL RECORRENTE - LSTM\n",
    "\n",
    "def create_rnn_lstm():\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nome = \"REDE NEURAL RECORRENTE - LSTM\"\n",
    "nome2 = \"LSTM\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_rnn_lstm()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_rnn_lstm()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_rnn_lstm()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Rede Neural Recorrente - GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDE NEURAL RECORRENTE - GRU\n",
    "\n",
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nome = \"REDE NEURAL RECORRENTE - GRU\"\n",
    "nome2 = \"GRU\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_rnn_gru()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_rnn_gru()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_rnn_gru()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Rede Neural Recorrente Bidirecional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDE NEURAL RECORRENTE BIDIRECIONAL\n",
    "\n",
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nome = \"REDE NEURAL RECORRENTE - BIDIRECIONAL\"\n",
    "nome2 = \"RNN-Bidirectional\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_bidirectional_rnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_bidirectional_rnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_bidirectional_rnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Rede Neural Recorrente Convolucional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDE NEURAL RECORRENTE CONVOLUCIONAL\n",
    "\n",
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((MAX_NB_WORDS, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(rnn_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    #output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nome = \"REDE NEURAL RECORRENTE CONVOLUCIONAL\"\n",
    "nome2 = \"RCNN\"\n",
    "\n",
    "print(\"\\n\"+nome)\n",
    "classifier = create_rcnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq, y_train, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2)\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Upsampling\")\n",
    "classifier = create_rcnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_UP, y_train_UP, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-UP\")\n",
    "\n",
    "\n",
    "print(\"\\n\"+nome+\" Downsampling\")\n",
    "classifier = create_rcnn()\n",
    "#classifier = models.load_model('CNN-glove-s50.h5')\n",
    "train_model_neural_network(classifier, X_train_seq_SUB, y_train_SUB, X_valid_seq, y_valid, X_test_seq, y_test, epochs=50, nome_modelo = nome2+\"-SUB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#Carrega base\n",
    "#df = pd.read_pickle('dados\\\\df_processado.pkl')\n",
    "# filtra tweets anotados, exclui so com classificacao D (delete)\n",
    "df2 = df[df['sent_manual'].fillna('nan').str.contains('N|E|S|C')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige_label(label):\n",
    "    if label == 'S' or label == 'E':\n",
    "        return('N')\n",
    "    else:\n",
    "        return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['sent_manual'] = df2['sent_manual'].apply(corrige_label)\n",
    "df2['sent_manual'].value_counts()\n",
    "lista_index = df.index.values.copy()\n",
    "lista_texto = df.tweet_limpo.to_list().copy()\n",
    "lista_label = df.sent_manual.to_list().copy()\n",
    "\n",
    "\n",
    "\n",
    "df2['sent_oplexicon3_ABP'] = df2['tweet_limpo'].apply(utils.oplexicon3_Absolute_Proportional_Difference)\n",
    "df2['oplexicon3'] = df2['sent_oplexicon3_ABP'].apply(utils.polaridade2)\n",
    "\n",
    "\n",
    "df2['tweet_lema'] = df2['tweet_para_traducao'].apply(utils.retorna_lemas)\n",
    "df2['sent_wordnetaffectbr_ABP'] = df2['tweet_lema'].apply(utils.wordnetaffectbr_Absolute_Proportional_Difference)\n",
    "df2['wordnetaffectbr'] = df2['sent_wordnetaffectbr_ABP'].apply(utils.polaridade2)\n",
    "\n",
    "df2['sent_LIWC_ABP'] = df2['tweet_limpo'].apply(utils.LIWC_Absolute_Proportional_Difference)\n",
    "df2['LIWC'] = df2['sent_LIWC_ABP'].apply(utils.polaridade2)\n",
    "\n",
    "df2['sent_SentiLex_ABP'] = df2['tweet_limpo'].apply(utils.SentiLex_Absolute_Proportional_Difference)\n",
    "df2['SentiLex'] = df2['sent_SentiLex_ABP'].apply(utils.polaridade2)\n",
    "\n",
    "df2['sent_Dicionario_conjunto'] = df2['tweet_limpo'].apply(utils.Dicionario_conjunto_Absolute_Proportional_Difference)\n",
    "df2['dic_conjunto'] = df2['sent_Dicionario_conjunto'].apply(utils.polaridade2)\n",
    "\n",
    "df2['ensemble_portugues'] = df2['tweet_limpo'].apply(utils.ensemble_portugues).apply(corrige_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparaResultados(serie, nome_modelo):\n",
    "    confusionMatrix(serie)\n",
    "    classificationReport(serie)\n",
    "    kappa = metrics.cohen_kappa_score(df2['sent_manual'],serie)\n",
    "    print(\"Kappa score: \",kappa,\"\\n\")\n",
    "    acc = metrics.accuracy_score(df2['sent_manual'],serie)\n",
    "    print(\"Accuracy score: \", acc,\"\\n\")\n",
    "    f1 = metrics.f1_score(y_true=df2['sent_manual'],y_pred=serie, average='weighted')\n",
    "    print(\"F1 micro score: \", f1,\"\\n\")\n",
    "    acc_bal = metrics.balanced_accuracy_score(df2['sent_manual'],serie)\n",
    "    print(\"Balanced Accuracy score: \", acc_bal,\"\\n\")\n",
    "    roc = metrics.roc_auc_score(df2['sent_manual']=='C',serie=='C')\n",
    "    print(\"Area under the ROC curve: {:.3f}\\n\".format(roc))\n",
    "    rec = metrics.recall_score(df2['sent_manual'],serie, pos_label = 'C', average='binary')\n",
    "    print(\"Recall classe C: {:.3f}\\n\".format(rec))\n",
    "    prec = metrics.precision_score(df2['sent_manual'],serie, pos_label = 'C', average='binary')\n",
    "    print(\"Precision classe C: {:.3f}\\n\".format(prec))\n",
    "\n",
    "    if nome_modelo:\n",
    "        dateTimeObj = datetime.now()\n",
    "        with open(\"Classificação de tweets\\\\resultados-classificacao.csv\", \"a\") as myfile:\n",
    "            myfile.write(nome_modelo+\",\"+str(serie.shape[0])+\",\"+str(acc)+\",\"+str(kappa)+\",\"+str(f1)+\",\"+str(acc_bal)+\",\"+str(roc)+\",\"+str(rec)+\",\"+str(prec)+\",\"+dateTimeObj.strftime(\"%Y-%m-%d\")+\"\\n\")\n",
    "    return \n",
    "\n",
    "\n",
    "def confusionMatrix(predictions):\n",
    "    #faz um processamento dos dados para uma melhor impressão\n",
    "    X = np.array( metrics.confusion_matrix(y_true=df2['sent_manual'],y_pred=predictions))\n",
    "    X = pd.DataFrame(X,index = ['C','N'], columns = ['C','N'])\n",
    "    print(X)\n",
    "    return\n",
    "\n",
    "# função que cria um relatório com base nas previsões realizadas pelo modelo\n",
    "def classificationReport(predictions):\n",
    "    print(metrics.classification_report(y_true=df2['sent_manual'],y_pred=predictions, target_names=['C','N']))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\noplexicon3:')\n",
    "comparaResultados(df2['oplexicon3'],'lex-oplexicon3')\n",
    "\n",
    "print('\\nwordnetaffectbr:')\n",
    "comparaResultados(df2['wordnetaffectbr'],'lex-wordnetaffectbr')\n",
    "\n",
    "print('\\nLIWC:')\n",
    "comparaResultados(df2['LIWC'],'lex-LIWC')\n",
    "\n",
    "print('\\nSentiLex:')\n",
    "comparaResultados(df2['SentiLex'],'lex-SentiLex')\n",
    "\n",
    "print('\\nDicionário Conjunto:')\n",
    "comparaResultados(df2['dic_conjunto'],'lex-dic_conjunto')\n",
    "\n",
    "print('\\nEnsemble Português:')\n",
    "comparaResultados(df2['ensemble_portugues'],'lex-ensemble_portugues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ensemble_portugues'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo SVM com todos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = df.sent_manual.to_list()\n",
    "y = encoder.transform(y_labels)\n",
    "print(len(corpus))\n",
    "#remove stopwords\n",
    "for i in range(0,len(corpus)): # varre a lista de textos\n",
    "    words=corpus[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in mystopwords] #remove as stop words\n",
    "    corpus[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "#tfidf\n",
    "tfidf_vect2 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                            ngram_range=(1, 1), max_features = max_tokens)\n",
    "tfidf_vect2.fit(corpus) # treina o objeto nos textos processados\n",
    "X_tfidf =  tfidf_vect2.transform(corpus)\n",
    "pickle.dump(tfidf_vect2.vocabulary_, open(\"dados/tfidf-vocab-full\", 'wb'))\n",
    "\n",
    "classifier = svm.SVC(probability=True)\n",
    "parameters = {'C': (0.0, 0.5, 1.0),\n",
    "               'kernel':('linear','poly','rbf','sigmoid'),\n",
    "               'class_weight' : ('balanced',None)}\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=0,cv=5)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_tfidf, y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    classifier = grid_search.best_estimator_\n",
    "\n",
    "joblib.dump(classifier, \"dados/class-SVM-tfidf-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
