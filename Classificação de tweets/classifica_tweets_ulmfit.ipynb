{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza o modelo pré treinado (pelo grupo deep learning BSB) da wikipedia português para treinar o classificador de tweets\n",
    "\n",
    "CLASSIFICAÇÃO BINÁRIA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device() # temque chamar essa função primeiro para evitar erro na inicializaçao do CUDA\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "path = os.getcwd()\n",
    "from sklearn import model_selection\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch: \", torch.__version__)\n",
    "print(\"fastai: \",__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('dados\\\\df_processado.pkl')\n",
    "df.info()\n",
    "#df = df[df['sent_manual'].fillna('nan').str.contains('N|E|S|C|D')]\n",
    "df2 = df[df['sent_manual'].fillna('nan').str.contains('N|E|S|C')].copy()\n",
    "#df[df['sent_manual'].str.contains('S|D')]['sent_manual'] = 'N'\n",
    "def corrige_label(label):\n",
    "    if label == 'S' or label == 'E':\n",
    "        return('N')\n",
    "    else:\n",
    "        return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['sent_manual'] = df2['sent_manual'].apply(corrige_label)\n",
    "df2['sent_manual'].value_counts()\n",
    "lista_index = df2.index.values.copy()\n",
    "lista_texto = df2.tweet_limpo.to_list().copy()\n",
    "lista_label = df2.sent_manual.to_list().copy()\n",
    "\n",
    "#TRANSFORMA PARA CAIXA BAIXA\n",
    "corpus = lista_texto.copy()\n",
    "for i in range(0,len(corpus)):\n",
    "    corpus[i]=corpus[i].lower()\n",
    "\n",
    "\n",
    "#REMOVE NUMEROS E PONTUACAO\n",
    "for i in range(0,len(corpus)):\n",
    "    corpus[i] = re.sub('[0-9]+', '', corpus[i])\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i])\n",
    "    corpus[i] = re.sub('º','',corpus[i])\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = corpus\n",
    "#trainDF['text'] = lista_texto\n",
    "trainDF['label'] = lista_label\n",
    "\n",
    "trainDF['label'].value_counts()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "test_size=0.2\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=test_size, random_state = 100)\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "        'texto' : train_x,\n",
    "        'label' : train_y\n",
    "        })\n",
    "\n",
    "df_valid = pd.DataFrame({\n",
    "        'texto' : valid_x,\n",
    "        'label' : valid_y\n",
    "        })\n",
    "\n",
    "print(\"df_train.shape =\", df_train.shape)\n",
    "print(\"df_valid.shape =\", df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando Vocabulário pré-treinado da Wiki em português treinada por integrante do curso NLP Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\models\\\\pt_wt_vocab.pkl', 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "data_lm = TextLMDataBunch.from_df(path, \n",
    "                                  train_df=df_train, \n",
    "                                  valid_df=df_valid,\n",
    "                                  text_cols=0, \n",
    "                                  label_cols=1, \n",
    "                                  vocab=vocab, \n",
    "                                  min_freq=3, \n",
    "                                  bs = batch_size )\n",
    "len(data_lm.vocab.itos),len(data_lm.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.vocab.itos[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o modelo de linguagem em cima do modelo de linguagem pré treinado da wikipedia pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, \n",
    "                                  AWD_LSTM, \n",
    "                                  drop_mult=0.3, \n",
    "                                  pretrained_fnames=['pt_wt', 'pt_wt_vocab']).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-1\n",
    "#learn_lm.fit_one_cycle(1, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('language_model_fastai_tweets_bc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = learn_lm.load('language_model_fastai_tweets_bc.pkl' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch\ttrain_loss\tvalid_loss\taccuracy\ttime \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch\ttrain_loss\tvalid_loss\taccuracy\ttime <br>\n",
    "0\t10.633375\t7.977017\t0.147321\t00:26 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_lm.fit_one_cycle(50, lr, moms=(0.8,0.7))\n",
    "learn_lm.fit_one_cycle(50, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('language_model_fastai_tweets_bc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save(f'pt_fine_tuned')\n",
    "learn_lm.save_encoder(f'pt_fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(trainDF, path, vocab=data_lm.vocab, cols='text')\n",
    "    .split_by_rand_pct(test_size, seed=100)\n",
    "    .label_from_df(cols='label')\n",
    "    .databunch(bs=batch_size, num_workers=1))\n",
    "\n",
    "data_clas.save(f'pt_textlist_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, f'pt_textlist_class', bs=batch_size, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, cohen_kappa_score\n",
    "\n",
    "@np_func\n",
    "def f1(inp,targ): return f1_score(targ, np.argmax(inp, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy,f1]).to_fp16()\n",
    "learn_c.load_encoder(f'pt_fine_tuned_enc')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=batch_size\n",
    "lr=2e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learn_c.fit_one_cycle(10, lr, moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_c.fit_one_cycle(5, lr, moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(5, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-2)\n",
    "#learn_c.fit_one_cycle(5, slice(lr/(2.6**4),lr), moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(5, slice(lr/(2.6**4),lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-3)\n",
    "#learn_c.fit_one_cycle(5, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(5, slice(lr/2/(2.6**4),lr/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_c.unfreeze()\n",
    "#learn_c.fit_one_cycle(3, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(3, slice(lr/10/(2.6**4),lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_c.unfreeze()\n",
    "#learn_c.fit_one_cycle(3, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))\n",
    "learn_c.fit_one_cycle(3, slice(lr/10/(2.6**4),lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save(f'pt_clas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, f'pt_textlist_class', bs=bs, num_workers=1)\n",
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy,f1]).to_fp16()\n",
    "learn_c.load(f'pt_clas', purge=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,targs = learn_c.get_preds(ordered=True)\n",
    "accuracy(preds,targs),f1(preds,targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.widgets import ClassConfusion\n",
    "interp = ClassificationInterpretation.from_learner(learn_c)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=interp.y_true,\n",
    "                            y_pred=interp.pred_class))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(interp.y_true,\n",
    "                  interp.pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, cohen_kappa_score, accuracy_score, balanced_accuracy_score, roc_auc_score, recall_score, precision_score\n",
    "from datetime import datetime\n",
    "\n",
    "y = interp.y_true\n",
    "predictions = interp.pred_class\n",
    "\n",
    "nome_modelo = \"ulmfit\"\n",
    "\n",
    "print(classification_report(y_true=y,\n",
    "                            y_pred=predictions))    \n",
    "\n",
    "#print(\"Kappa score: \", cohen_kappa_score(predictions, y),\"\\n\")\n",
    "#print(\"Accuracy score: \", accuracy_score(predictions, y),\"\\n\")\n",
    "#print(\"f1 macro score: \", f1_score(predictions, y, average='macro'),\"\\n\")\n",
    "#print(\"Balanced Accuracy score: \", balanced_accuracy_score(predictions, y))\n",
    "\n",
    "kappa = cohen_kappa_score(y, predictions)\n",
    "print(\"Kappa score: {:.3f}\\n\".format(kappa))\n",
    "acc = accuracy_score(y, predictions)\n",
    "print(\"Accuracy score: {:.3f}\\n\".format(acc))\n",
    "f1 = f1_score(y, predictions, average='weighted')\n",
    "print(\"f1 macro score: {:.3f}\\n\".format(f1))\n",
    "acc_bal = balanced_accuracy_score(y, predictions)\n",
    "print(\"Balanced Accuracy score: {:.3f}\\n\".format(acc_bal))\n",
    "roc = roc_auc_score(y, predictions)\n",
    "print(\"Area under the ROC curve: {:.3f}\\n\".format(roc))\n",
    "rec = recall_score(y, predictions, pos_label = 0, average='binary')\n",
    "print(\"Recall classe C: {:.3f}\\n\".format(rec))\n",
    "prec = precision_score(y, predictions, pos_label = 0, average='binary')\n",
    "print(\"Recall classe C: {:.3f}\\n\".format(prec))\n",
    "\n",
    "\n",
    "if nome_modelo:\n",
    "    dateTimeObj = datetime.now()\n",
    "    with open(\"Classificação de tweets\\\\resultados-classificacao.csv\", \"a\") as myfile:\n",
    "        myfile.write(nome_modelo+\",\"+str(y.shape[0])+\",\"+str(acc)+\",\"+str(kappa)+\",\"+str(f1)+\",\"+str(acc_bal)+\",\"+str(roc)+\",\"+str(rec)+\",\"+str(prec)+\",\"+dateTimeObj.strftime(\"%Y-%m-%d\")+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
