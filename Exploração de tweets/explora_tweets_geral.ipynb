{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração de Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARREGA DADOS E PACOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import networkx as nx \n",
    "from networkx.algorithms.community import k_clique_communities, girvan_newman, greedy_modularity_communities\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "import spacy\n",
    "import scattertext as st\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "plt.rcParams[\"figure.figsize\"] = (60,30)\n",
    "plt.rcParams['figure.dpi'] = 90\n",
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mystopwords = utils.mystopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('dados\\\\df_processado.pkl')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de Tweets:\", df[df['data']>\"2020-03-01\"].shape[0])\n",
    "print(\"Total de Usuários:\", df['usuario'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleciona usuários com mais de 10 tweets publicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['data']>\"2020-03-01\"]\n",
    "df = df[~df['sent_manual'].isin(['D'])] #remove tweets marcadaos com D (delete)\n",
    "df = df[~df['usuario'].isin(['BancoCentralBR'])] #remove BancoCentralBR\n",
    "\n",
    "Nr_min_tweets = 10\n",
    "\n",
    "lista_usuarios = df['usuario'].value_counts()[df['usuario'].value_counts()>Nr_min_tweets].index.values.tolist()\n",
    "print(\"Número de usuários com mais de\", Nr_min_tweets,\"Tweets:\",len(lista_usuarios))\n",
    "df = df[df['usuario'].isin(lista_usuarios)]\n",
    "\n",
    "\n",
    "lista_usuarios2 = lista_usuarios\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE TWEETS_LIMPOS SEM TOKENS \n",
    "df=df[df['tweet_limpo'].str.len()>0]\n",
    "# REMOVE TWEETS COM POUCAS PALAVRAS\n",
    "df = df[df['tweet_limpo'].str.split().apply(lambda x: len(x))>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de Tweets:\", df[df['data']>\"2020-03-01\"].shape[0])\n",
    "print(\"Total de Usuários:\", df['usuario'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleciona a maior comunidade da rede de menções e retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df[df['usuario'].isin(lista_usuarios2)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de Tweets:\", df[df['data']>\"2020-03-01\"].shape[0])\n",
    "print(\"Total de Usuários:\", df['usuario'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data2'] = df['data'].apply(lambda x: x.date())\n",
    "df['data2'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data2'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de dias:\", (np.max(df['data2'])-np.min(df['data2'])).days)\n",
    "print(\"Número médio de tweets por dia:\", df.shape[0]/(np.max(df['data2'])-np.min(df['data2'])).days)\n",
    "print(\"Número de semanas:\", (np.max(df['data2'])-np.min(df['data2'])).days//7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ANÁLISE DESCRITIVA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MÉTRICAS DE TWEETS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### por dia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('média: ', df['data'].apply(lambda x: x.date()).value_counts().mean())\n",
    "print('desvio padrão: ', df['data'].apply(lambda x: x.date()).value_counts().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'].apply(lambda x: x.date()).value_counts().sort_index().asfreq('D').fillna(0).plot(linewidth = 8, title = \"nr de tweets por dia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograma das publicações diárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'].apply(lambda x: x.date()).value_counts().sort_index().asfreq('D').fillna(0).hist(bins = 20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "df['data'].astype(np.int64).plot.hist(ax=ax)\n",
    "labels = ax.get_xticks().tolist()\n",
    "labels = pd.to_datetime(labels)\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### por dia e mês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['data'].dt.day\n",
    "df['month'] = df['data'].dt.month\n",
    "\n",
    "# -------------------------- Heat map ------------------------------\n",
    "df1 = df.groupby([\"day\", \"month\"]).count().reset_index()\n",
    "hm = df1.pivot(\"day\", \"month\", \"tweet\")\n",
    "ax = sns.heatmap(hm, cmap=sns.cm.rocket_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle' | 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.concatenate((x, [x[0]]))\n",
    "                y = np.concatenate((y, [y[0]]))\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def draw(self, renderer):\n",
    "            \"\"\" Draw. If frame is polygon, make gridlines polygon-shaped \"\"\"\n",
    "            if frame == 'polygon':\n",
    "                gridlines = self.yaxis.get_gridlines()\n",
    "                for gl in gridlines:\n",
    "                    gl.get_path()._interpolation_steps = num_vars\n",
    "            super().draw(renderer)\n",
    "\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "\n",
    "\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "lista_dados1 = df[df['month']==3]['data'].dt.day.value_counts().sort_index().values.tolist()\n",
    "lista_dados2 = df[df['month']==4]['data'].dt.day.value_counts().sort_index().values.tolist()+['0']\n",
    "lista_dados3 = df[df['month']==5]['data'].dt.day.value_counts().sort_index().values.tolist()\n",
    "lista_dados4 = df[df['month']==6]['data'].dt.day.value_counts().sort_index().values.tolist()+['0']\n",
    "lista_dados5 = df[df['month']==7]['data'].dt.day.value_counts().sort_index().values.tolist()\n",
    "lista_dados6 = df[df['month']==8]['data'].dt.day.value_counts().sort_index().values.tolist()\n",
    "lista_dados6 = lista_dados6+['0' for i in range(31-len(lista_dados6))]\n",
    "\n",
    "lista_dados1.reverse()\n",
    "lista_dados2.reverse()\n",
    "lista_dados3.reverse()\n",
    "lista_dados4.reverse()\n",
    "lista_dados5.reverse()\n",
    "lista_dados6.reverse()\n",
    "lista_dias = [str(i) for i in range(1,32)]\n",
    "lista_dias.reverse()\n",
    "\n",
    "data = [lista_dias,\n",
    "        ('Tweets por dia do mês', [\n",
    "            lista_dados1,\n",
    "            lista_dados2,\n",
    "            lista_dados3,\n",
    "            lista_dados4,\n",
    "            lista_dados5,\n",
    "            lista_dados6])]\n",
    "\n",
    "N = len(data[0])\n",
    "theta = radar_factory(N, frame='polygon')\n",
    "\n",
    "spoke_labels = data.pop(0)\n",
    "title, case_data = data[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(top=0.85, bottom=0.05)\n",
    "\n",
    "ax.set_rgrids([50, 100, 150, 200])\n",
    "ax.set_title(title,  position=(0.5, 1.1), ha='center')\n",
    "\n",
    "for d in case_data:\n",
    "    line = ax.plot(theta, d)\n",
    "    ax.fill(theta, d,  alpha=0.25)\n",
    "ax.set_varlabels(spoke_labels)\n",
    "\n",
    "# add legend relative to top-left plot\n",
    "labels = ('Março', 'Abril', 'Maio', 'Junho','julho','agosto')\n",
    "plt.legend(labels, loc=(0.9, .95), labelspacing=0.1, fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### por dia da semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'].dt.dayofweek\\\n",
    ".apply(lambda x: \"segunda\" if x==0 else \"terça\" if x==1 else \"quarta\" if x==2 else \"quinta\" if x==3 else \"sexta\" if x==4 else \"sábado\" if x==5 else \"domingo\" )\\\n",
    ".value_counts().reindex([\"domingo\",\"segunda\",\"terça\",\"quarta\",\"quinta\",\"sexta\",\"sábado\"]).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "lista_dados = df['data'].dt.dayofweek\\\n",
    ".apply(lambda x: \"segunda\" if x==0 else \"terça\" if x==1 else \"quarta\" if x==2 else \"quinta\" if x==3 else \"sexta\" if x==4 else \"sábado\" if x==5 else \"domingo\" )\\\n",
    ".value_counts().reindex([\"domingo\",\"segunda\",\"terça\",\"quarta\",\"quinta\",\"sexta\",\"sábado\"]).values.tolist()\n",
    "\n",
    "lista_dados.reverse()\n",
    "lista_dias_semana = ['Domingo', 'Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado']\n",
    "lista_dias_semana.reverse()\n",
    "\n",
    "data = [lista_dias_semana,\n",
    "        ('Tweets por dia da semana', [\n",
    "            lista_dados])]\n",
    "\n",
    "N = len(data[0])\n",
    "theta = radar_factory(N, frame='polygon')\n",
    "\n",
    "spoke_labels = data.pop(0)\n",
    "title, case_data = data[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(top=0.85, bottom=0.05)\n",
    "\n",
    "ax.set_rgrids([200, 400, 600, 800])\n",
    "ax.set_title(title,  position=(0.5, 1.1), ha='center')\n",
    "\n",
    "for d in case_data:\n",
    "    line = ax.plot(theta, d)\n",
    "    ax.fill(theta, d,  alpha=0.25)\n",
    "ax.set_varlabels(spoke_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acumulado por hora do dia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['data'].apply(lambda x: x.hour)\n",
    "df['data'].dt.hour.value_counts().sort_index().plot(linewidth = 10, title = \"nr de tweets acumulados por hora do dia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "lista_dados = df['data'].dt.hour.value_counts().sort_index().values.tolist()\n",
    "\n",
    "lista_dados.reverse()\n",
    "lista_horas = ['0h','1h','2h','3h','4h','5h','6h','7h','8h','9h','10h','11h','12h','13h','14h','15h','16h','17h','18h','19h','20h','21h','22h','23h']\n",
    "lista_horas.reverse()\n",
    "\n",
    "data = [lista_horas,\n",
    "        ('Tweets por hora do dia', [\n",
    "            lista_dados])]\n",
    "\n",
    "N = len(data[0])\n",
    "theta = radar_factory(N, frame='polygon')\n",
    "\n",
    "spoke_labels = data.pop(0)\n",
    "title, case_data = data[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(top=0.85, bottom=0.05)\n",
    "\n",
    "ax.set_rgrids([100, 200, 300, 400, 500])\n",
    "ax.set_title(title,  position=(0.5, 1.1), ha='center')\n",
    "\n",
    "for d in case_data:\n",
    "    line = ax.plot(theta, d)\n",
    "    ax.fill(theta, d,  alpha=0.25)\n",
    "ax.set_varlabels(spoke_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### por hora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*df[df['data']>\"2020-03-01\"]['data'].shape[0]\n",
    "# the index of the series\n",
    "idx = pd.DatetimeIndex(df[df['data']>\"2020-03-01\"]['data'])\n",
    "# Resampling / bucketing\n",
    "pd.Series(ones, index=idx).resample('H').sum().fillna(0).plot(title = \"Nr de tweets por hora\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograma das publicações por hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ones, index=idx).resample('H').sum().fillna(0).hist(bins = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### número de likes por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['data2','likes']].groupby('data2').sum().plot(xlim=('2020-03-01',np.max(df['data'])),linewidth = 10, title = \"Nr likes por dia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### número de retweets por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['data2','retweets']].groupby('data2').sum().plot(xlim=('2020-03-01',np.max(df['data'])),linewidth = 10, title = \"Nr retweets por dia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'].value_counts().sort_values(ascending=False).head(5).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maior número de likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='likes', ascending=False)[['data2','usuario','tipo_usuario','seguidores','likes','retweets','tweet']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maior número de retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='retweets', ascending=False)[['data2','usuario','tipo_usuario','seguidores','likes','retweets','tweet']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maior número de respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['replies'].max()>0:\n",
    "    df.sort_values(by='replies', ascending=False)[['data2','usuario','tipo_usuario','seguidores','likes','retweets','replies','tweet']].head(5)\n",
    "else:\n",
    "    print(\"Não há tweets com replies > 0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comprimento em caracteres do tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comprimento médio: \", df['tweet'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### número de palavras por tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número médio de palavras por tweet\", df['tweet'].str.split().map(lambda x: len(x)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].str.split().\\\n",
    "    map(lambda x: len(x)).\\\n",
    "    hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de palavras publicadas por dia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nr_palavras'] = df['tweet_limpo'].apply(lambda x: len(x.split(\" \")))\n",
    "df[['data2','nr_palavras']].groupby('data2').sum().plot(xlim=('2020-03-01',np.max(df['data'])),linewidth = 10, title = \"Nr palavras publicadas por dia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MÉTRICAS DE USUÁRIOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total de usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['usuario'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lista_usuarios = df['usuario'].unique().tolist()\n",
    "lista_usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usuários ativos por dia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['data2','usuario']].drop_duplicates()['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])),linewidth = 10,title = \"Número de usuários ativos por dia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets por usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['usuario'].value_counts().sort_values(ascending=False).head(20).plot.bar(title = \"nr de tweets por usuários\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com mais retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','retweets']].groupby('usuario').sum().sort_values(by = \"retweets\",ascending=False).head(20).plot.bar(title = \"Retweets por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com maiores médias de retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','retweets']].groupby('usuario').mean().sort_values(by = \"retweets\",ascending=False).head(20).plot.bar(title = \"Média de Retweets por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com mais likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','likes']].groupby('usuario').sum().sort_values(by = \"likes\",ascending=False).head(20).plot.bar(title = \"Likes por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com maiores médias de likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','likes']].groupby('usuario').mean().sort_values(by = \"likes\",ascending=False).head(20).plot.bar(title = \"Média de Likes por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usuários com mais seguidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','seguidores']].groupby('usuario').max().sort_values(by = 'seguidores',ascending=False).head(20).plot.bar(title = \"nr de seguidores por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usuários com mais amigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','amigos']].groupby('usuario').max().sort_values(by = 'amigos',ascending=False).head(20).plot.bar(title = \"nr de amigos por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow ratio\n",
    "\n",
    "Razão entre o número de seguidores e o de seguidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['follow_ratio'] = df['seguidores']/df['amigos'] #risco de dar divisão por zero\n",
    "df['follow_ratio']=df['follow_ratio'].apply(lambda x: np.nan if x==np.inf else x) #remove os infinitos gerados pela divisão por zero\n",
    "df[['usuario','follow_ratio']].groupby('usuario').max().sort_values(by = 'follow_ratio',ascending=False).head(20).plot.bar(title = \"Follow ratio por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com mais publicações (statuses) \n",
    "\n",
    "Mede a atividade do usuário na rede social. Quantos tweets o usuário publicou no total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','nr_tweets']].groupby('usuario').max().sort_values(by = 'nr_tweets',ascending=False).head(20).plot.bar(title = \"nr de tweets publicados (statuses) por usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuários com mais favoritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['usuario','favoritos']].groupby('usuario').max().sort_values(by = 'favoritos',ascending=False).head(20).plot.bar(title = \"nr de tweets considerados favoritos pelo usuário\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usuários mais citados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet.str.extractall(r'(\\@\\w+)')[0].value_counts().head(30).plot.bar(title = \"usuários mais citados nos tweets\")\n",
    "print(\"Número de usuários citados:\", df.tweet.str.extractall(r'(\\@\\w+)')[0].value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usuários da base mais citados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.tweet.str.extractall(r'(\\@\\w+)')[0].value_counts()\n",
    "s[s.index.str.contains('|'.join(lista_usuarios2))].head(30).plot.bar(title = \"usuários economistas e jornalistas mais citados\")\n",
    "print(\"Número de usuários economistas citados:\", s[s.index.str.contains('|'.join(lista_usuarios2))].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tipos de usuários "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_usuario'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['tipo_usuario'].value_counts().sort_values(ascending=False).plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_usuario'].value_counts().sort_values(ascending=False).plot.bar(title = \"nr de usuários por tipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('tipo_usuario')[['seguidores','likes','retweets']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localidades dos usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['localidade'].value_counts().to_frame().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição dos usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequência de palavras das descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vect3 = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words = Mystopwords+['of','rio','and','are','my','co','own','https','the','t','ex','at','in','freeassange','freepalestine','4v5qxkjosj','acompanhe','sauvyyjbfe'])\n",
    "word_count_vect3.fit( [x for x in  df['description'].unique().tolist()  if x is not None]  );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words3 = word_count_vect3.transform([x for x in  df['description'].unique().tolist()  if x is not None] )\n",
    "sum_words = bag_of_words3.sum(axis=0)\n",
    "words_freq3 = [(word, sum_words[0, idx]) for word, idx in word_count_vect3.vocabulary_.items()]\n",
    "words_freq3 =sorted(words_freq3, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 20\n",
    "y_pos = np.arange(number_of_words)\n",
    "objects = []\n",
    "performance = []\n",
    "for i in range(number_of_words):\n",
    "    aux = words_freq3[i]\n",
    "    objects.append(aux[0])\n",
    "    performance.append(aux[1])\n",
    "del number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Frequência')\n",
    "plt.ylabel('Palavras das descrições')\n",
    "plt.title('Frequência de palavras das descrições de usuários sem stop words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuvem de palavras das descrições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(stopwords=Mystopwords+['de','da','do','em']+['of','rio','and','are','my','co','own','https','the','t','ex','at','in','freeassange','freepalestine','4v5qxkjosj','acompanhe','sauvyyjbfe','http','and'], max_font_size=50, max_words=100, background_color=\"white\").\\\n",
    "generate(' '.join( [x for x in  df['description'].unique().tolist()  if x is not None] ).lower())\n",
    "# Display the generated image:\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associação entre citações de usuários\n",
    "\n",
    "* __Support__: This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.\n",
    "* __Confidence__: This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears.\n",
    "* __Lift__: This says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought.\n",
    "\n",
    "Fonte: https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_lista_de_usuarios=[]\n",
    "for tweet in df['tweet']:\n",
    "    l = re.findall(r'(\\@\\w+)',tweet)\n",
    "    if len(l)>1:\n",
    "        lista_de_lista_de_usuarios.append(l)\n",
    "\n",
    "#aplica o algoritmo apriori\n",
    "itemsets, rules = apriori(lista_de_lista_de_usuarios, min_support=0.01,  min_confidence=0.4)\n",
    "rules_rhs = filter(lambda rule: len(rule.lhs) == 1 and len(rule.rhs) == 1, rules)\n",
    "for rule in sorted(rules_rhs, key=lambda rule: rule.lift):\n",
    "    print(rule) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering de usuários\n",
    "\n",
    "Baseado em https://github.com/twitterdev/do_more_with_twitter_data/blob/master/examples/clustering_users/clustering-users.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(preprocessor=utils.remove_url, \n",
    "                      stop_words=Mystopwords+\\\n",
    "                      ['of','rio','and','are','my','co','own','https','the','t','ex','at','rt','nunca','tudo',\\\n",
    "                      'alvaro','si'],\n",
    "                      \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df[['usuario','description']].copy()\n",
    "df_aux = df_aux.drop_duplicates(subset = ['usuario'],keep = 'last')\n",
    "\n",
    "unique_user_map = {}\n",
    "# create one entry per user\n",
    "for i in range(df_aux.shape[0]):\n",
    "    unique_user_map[df_aux['usuario'].values.tolist()[i]] = df_aux['description'].values.tolist()[i].lower()\n",
    "\n",
    "\n",
    "# we need to maintain the same ordering of users and bios\n",
    "unique_users = []\n",
    "unique_bios = []\n",
    "for user,bio in unique_user_map.items():\n",
    "    unique_users.append(user)\n",
    "    if bio is None:\n",
    "        # special case for empty bios\n",
    "        bio = ''\n",
    "    unique_bios.append(bio)\n",
    "\n",
    "del df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_matrix = vec.fit_transform(unique_bios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=10,\n",
    "                               prediction_data=True,\n",
    "                               core_dist_n_jobs=-1,\n",
    "                               memory='data')\n",
    "hdbs.fit(bio_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the population sizes \n",
    "label_counts = Counter(hdbs.labels_)\n",
    "xs, ys = [], []\n",
    "for k,v in label_counts.items():\n",
    "    xs.append(k)\n",
    "    ys.append(v)\n",
    "\n",
    "# draw the chart\n",
    "plt.bar(xs, ys)\n",
    "\n",
    "plt.xticks(range(-1, len(label_counts)))\n",
    "plt.ylabel('population')\n",
    "plt.xlabel('cluster label')\n",
    "plt.title('population sizes ({} clusters found by hdbscan)'.format(len(label_counts) - 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongest_features(model, vectorizer, matrix, topk=10):\n",
    "    \"\"\"\n",
    "    Helper function to display a simple text representation of the top-k most \n",
    "    important features in our fit model and vectorizer.\n",
    "    \n",
    "    matrix: matriz obtida da função vec.fit_transform\n",
    "    vectorizer: sklearn vectorizer\n",
    "    topk: k numbers of words to get per cluster\n",
    "    \n",
    "    \"\"\"\n",
    "    features = vectorizer.get_feature_names()\n",
    "    # ignore noise labels\n",
    "    relevant_labels = [ x for x in set(model.labels_) if x >= 0 ]\n",
    "    for this_label in relevant_labels:\n",
    "        matching_rows = np.where(hdbs.labels_ == this_label)[0]\n",
    "        coeff_sums = np.sum(matrix[matching_rows], axis=0).A1\n",
    "        sorted_coeff_idxs = np.argsort(coeff_sums)[::-1]\n",
    "        print('Cluster {}: '.format(this_label), end='')\n",
    "        for idx in sorted_coeff_idxs[:topk]:\n",
    "            print('{} '.format(features[idx]), end='')\n",
    "        print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongest_features_per_cluster(cluster, model, vectorizer, matrix, topk=10):\n",
    "    \"\"\"\n",
    "    Helper function to display a simple text representation of the top-k most \n",
    "    important features in our fit model and vectorizer.\n",
    "    \n",
    "    cluster: cluster number\n",
    "    matrix: matriz obtida da função vec.fit_transform\n",
    "    vectorizer: sklearn vectorizer\n",
    "    topk: k numbers of words to get per cluster\n",
    "    \n",
    "    \"\"\"\n",
    "    features = vectorizer.get_feature_names()\n",
    "    # ignore noise labels\n",
    "    relevant_labels = [ x for x in set(model.labels_) if x >= 0 ]\n",
    "    this_label = cluster\n",
    "    matching_rows = np.where(hdbs.labels_ == this_label)[0]\n",
    "    coeff_sums = np.sum(matrix[matching_rows], axis=0).A1\n",
    "    sorted_coeff_idxs = np.argsort(coeff_sums)[::-1]\n",
    "    print('Cluster {}: '.format(this_label), end='')\n",
    "    for idx in sorted_coeff_idxs[:topk]:\n",
    "        print('{} '.format(features[idx]), end='')\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strongest_features(hdbs, vec, bio_matrix, topk=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 MÉTRICAS DE URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possui links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].apply(lambda x: \"https://t.co\" in x).value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites mais compartilhados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#carrega dicionário de urls\n",
    "pickle_in = open(\"dados\\\\dict_url.pickle\",\"rb\")\n",
    "dict_url = pickle.load(pickle_in)\n",
    "#pega lista de urls dos tweets\n",
    "lista_urls = df.tweet.str.extractall(r'(http\\S+)')[0].values.tolist()\n",
    "lista_real_urls = []\n",
    "for shorturl in lista_urls:\n",
    "    if shorturl in dict_url.keys():\n",
    "        lista_real_urls.append(dict_url[shorturl])\n",
    "    else:\n",
    "        lista_real_urls.append(\"NA\")\n",
    "        \n",
    "\n",
    "\n",
    "lista_domain = []\n",
    "for url in lista_real_urls:\n",
    "    if url != \"NA\":\n",
    "        lista_domain.append(url.split(\"//\")[-1].split(\"/\")[0].split('?')[0])\n",
    "\n",
    "print(\"Número total de urls:\",len(lista_urls))\n",
    "print(\"Número de urls válidas:\",len(lista_domain),\"\\n\")\n",
    "        \n",
    "domains = Counter(lista_domain)\n",
    "domains.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "for key, value in domains.most_common(15):\n",
    "    keys.append(key)\n",
    "    values.append(value)\n",
    "\n",
    "y_pos = np.arange(len(keys))\n",
    "\n",
    "plt.barh(y_pos, values, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, keys)\n",
    "plt.xlabel('Número de compartilhamentos')\n",
    "plt.title('Domínio das urls mais compartilhadas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ANÁLISE DE CONTEÚDO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ANÁLISE DE PALAVRAS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vect2 = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words = Mystopwords)\n",
    "word_count_vect2.fit(df['tweet_limpo'].to_list());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words2 = word_count_vect2.transform(df['tweet_limpo'].to_list())\n",
    "sum_words = bag_of_words2.sum(axis=0)\n",
    "words_freq2 = [(word, sum_words[0, idx]) for word, idx in word_count_vect2.vocabulary_.items()]\n",
    "words_freq2 =sorted(words_freq2, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 20\n",
    "y_pos = np.arange(number_of_words)\n",
    "objects = []\n",
    "performance = []\n",
    "for i in range(number_of_words):\n",
    "    aux = words_freq2[i]\n",
    "    objects.append(aux[0])\n",
    "    performance.append(aux[1])\n",
    "del number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Frequência')\n",
    "plt.ylabel('Palavras')\n",
    "plt.title('Frequência de palavras sem stop words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vect2 = CountVectorizer(analyzer='word', ngram_range=(2, 2),stop_words = Mystopwords)\n",
    "bigram_count_vect2.fit(df['tweet_limpo'].to_list());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_bigrams2 = bigram_count_vect2.transform(df['tweet_limpo'].to_list())\n",
    "sum_bigrams = bag_of_bigrams2.sum(axis=0)\n",
    "bigrams_freq2 = [(bigram, sum_bigrams[0, idx]) for bigram, idx in bigram_count_vect2.vocabulary_.items()]\n",
    "bigrams_freq2 =sorted(bigrams_freq2, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bigrams = 20\n",
    "y_pos = np.arange(number_of_bigrams)\n",
    "objects = []\n",
    "performance = []\n",
    "for i in range(number_of_bigrams):\n",
    "    aux = bigrams_freq2[i]\n",
    "    objects.append(aux[0])\n",
    "    performance.append(aux[1])\n",
    "del number_of_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.title('Bigrams Frequency without stop words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_count_vect2 = CountVectorizer(analyzer='word', ngram_range=(3, 3),stop_words = Mystopwords)\n",
    "trigram_count_vect2.fit(df['tweet_limpo'].to_list());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_trigrams2 = trigram_count_vect2.transform(df['tweet_limpo'].to_list())\n",
    "sum_trigrams = bag_of_trigrams2.sum(axis=0)\n",
    "trigrams_freq2 = [(trigram, sum_trigrams[0, idx]) for trigram, idx in trigram_count_vect2.vocabulary_.items()]\n",
    "trigrams_freq2 =sorted(trigrams_freq2, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trigrams = 20\n",
    "y_pos = np.arange(number_of_trigrams)\n",
    "objects = []\n",
    "performance = []\n",
    "for i in range(number_of_trigrams):\n",
    "    aux = trigrams_freq2[i]\n",
    "    objects.append(aux[0])\n",
    "    performance.append(aux[1])\n",
    "del number_of_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Trigrams')\n",
    "plt.title('Trigrams Frequency without stop words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4gram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgram_count_vect2 = CountVectorizer(analyzer='word', ngram_range=(4, 4),stop_words = Mystopwords)\n",
    "fourgram_count_vect2.fit(df['tweet_limpo'].to_list());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_fourgrams2 = fourgram_count_vect2.transform(df['tweet_limpo'].to_list())\n",
    "sum_fourgrams = bag_of_fourgrams2.sum(axis=0)\n",
    "fourgrams_freq2 = [(fourgram, sum_fourgrams[0, idx]) for fourgram, idx in fourgram_count_vect2.vocabulary_.items()]\n",
    "fourgrams_freq2 =sorted(fourgrams_freq2, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_fourgrams = 20\n",
    "y_pos = np.arange(number_of_fourgrams)\n",
    "objects = []\n",
    "performance = []\n",
    "for i in range(number_of_fourgrams):\n",
    "    aux = fourgrams_freq2[i]\n",
    "    objects.append(aux[0])\n",
    "    performance.append(aux[1])\n",
    "del number_of_fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('4-grams')\n",
    "plt.title('4-+ grams Frequency without stop words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvens de Palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(stopwords=Mystopwords+['de','da'], max_font_size=50, max_words=100, background_color=\"white\").\\\n",
    "generate(' '.join(df['tweet_limpo'].to_list()).lower())\n",
    "# Display the generated image:\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anotados como crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(stopwords=Mystopwords, max_font_size=50, max_words=100, background_color=\"white\").\\\n",
    "generate(' '.join(df[df['sent_manual']=='C']['tweet_limpo'].to_list()).lower())\n",
    "# Display the generated image:\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_words = TfidfVectorizer(stop_words=Mystopwords+\\\n",
    "                      ['of','rio','and','are','my','co','own','https','the','t','ex','at','rt','nunca','tudo',\\\n",
    "                      'alvaro','si'],\n",
    "                             #max_df=1.0,\n",
    "                             min_df=80,\n",
    "                             #max_features=4000,\n",
    "                      \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_matrix = vec_words.fit_transform(df['tweet_limpo'])\n",
    "words_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_matrix = words_matrix.transpose()\n",
    "words_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(18, 16))\n",
    "#plt.title(\"Word Dendogram\")\n",
    "#dend = shc.dendrogram(shc.linkage(words_matrix.todense(), method='ward'),\n",
    "#                      labels=vec_words.get_feature_names())\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Word Dendogram\")\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "dend = shc.dendrogram(shc.linkage(words_matrix.todense(), method='ward'),\n",
    "                      labels=vec_words.get_feature_names(),ax=ax)\n",
    "\n",
    "ax.tick_params(axis='x', which='major', labelsize=20)\n",
    "ax.tick_params(axis='y', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/JasonKessler/scattertext  #diversos exemplos\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "corpus = st.CorpusFromPandas(df[df['sent_manual'].isin(['C','N'])],\n",
    "                             category_col='sent_manual',\n",
    "                             text_col='tweet_limpo',\n",
    "                             nlp=nlp).build().remove_terms(Mystopwords, ignore_absences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "                                       category='C',\n",
    "                                       category_name='Críticas',\n",
    "                                       not_category_name='Não Críticas',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       )\n",
    "open(\"Convention-Visualization-geral.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML(filename='Convention-Visualization-geral.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#i = df.index[0]\n",
    "#text =  np.array2string( df.loc[[i],['tweet_limpo']].values)\n",
    "text = \" \".join(df['tweet_limpo'].to_list())\n",
    "bigrams = [(a, b) for l in [text] for a,b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Create a network plot of grouped terms\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(50),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "# Create dictionary of bigrams and their counts\n",
    "d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "# Create network plot \n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for k, v in d[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=5.5)\n",
    "#pos = nx.kamada_kawai_layout(G)\n",
    "#pos = nx.circular_layout(G)\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=3,\n",
    "                 edge_color='gray',\n",
    "                 node_color='purple',\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.135, value[1]+.05\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='red', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_emoji = []\n",
    "for txt in df['tweet']:\n",
    "    lista = split_count(txt)\n",
    "    if len(lista)>0:\n",
    "        lista_emoji.append(lista)\n",
    "lista_emoji = [item for sublist in lista_emoji for item in sublist]\n",
    "lista_emoji\n",
    "\n",
    "emoji_counts = Counter(lista_emoji)\n",
    "\n",
    "# Create a network plot of grouped terms\n",
    "pd.DataFrame(emoji_counts.most_common(100), columns=['emoji', 'count'])\n",
    "#matplotlib não plota emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ANÁLISE DE HASHTAGS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet.str.extractall(r'(\\#\\w+)')[0].value_counts().head(30).plot.bar()\n",
    "print(\"Hashtags únicas:\",df.tweet.str.extractall(r'(\\#\\w+)')[0].value_counts().shape[0])\n",
    "print(\"Total de Hashtags:\",df.tweet.str.extractall(r'(\\#\\w+)')[0].value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associação entre hashtags \n",
    "\n",
    "* __Support__: This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.\n",
    "* __Confidence__: This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears.\n",
    "* __Lift__: This says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought.\n",
    "\n",
    "Fonte: https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_de_lista_dehashtags=[]\n",
    "for tweet in df['tweet']:\n",
    "    l = re.findall(r'(\\#\\w+)',tweet)\n",
    "    if len(l)>1:\n",
    "        lista_de_lista_dehashtags.append(l)\n",
    "\n",
    "#aplica o algoritmo apriori\n",
    "itemsets, rules = apriori(lista_de_lista_dehashtags, min_support=0.05,  min_confidence=0.5)\n",
    "rules_rhs = filter(lambda rule: len(rule.lhs) == 1 and len(rule.rhs) == 1, rules)\n",
    "for rule in sorted(rules_rhs, key=lambda rule: rule.lift):\n",
    "    print(rule) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ANÁLISE DE SENTIMENTOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de dicionários "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polaridade(valor):\n",
    "    if valor>0.02:\n",
    "        return(\"positivo\")\n",
    "    elif valor<-0.02:\n",
    "        return(\"negativo\")\n",
    "    else:\n",
    "        return(\"neutro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'oplexicon3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'sent_oplexicon3_ABP'\n",
    "df[col].apply(polaridade).value_counts().plot.bar(title = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df[['data2', col]].copy()\n",
    "df_aux['polaridade'] = df_aux[col].apply(polaridade)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='positivo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Positivo', ax=ax, color = 'blue')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='neutro']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Neutro', ax=ax, color = 'black')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='negativo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Negativo', ax=ax, color = 'red')\n",
    "\n",
    "ax.legend(['Positivo', 'Neutro', 'Negativo'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'SentiLex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'sent_SentiLex_ABP'\n",
    "df[col].apply(polaridade).value_counts().plot.bar(title = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df[['data2', col]].copy()\n",
    "df_aux['polaridade'] = df_aux[col].apply(polaridade)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='positivo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Positivo', ax=ax, color = 'blue')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='neutro']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Neutro', ax=ax, color = 'black')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='negativo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Negativo', ax=ax, color = 'red')\n",
    "\n",
    "ax.legend(['Positivo', 'Neutro', 'Negativo'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'textblob_polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'textblob_polarity'\n",
    "df[col].apply(polaridade).value_counts().plot.bar(title = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df[['data2', col]].copy()\n",
    "df_aux['polaridade'] = df_aux[col].apply(polaridade)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='positivo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Positivo', ax=ax, color = 'blue')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='neutro']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Neutro', ax=ax, color = 'black')\n",
    "\n",
    "df_aux[['data2', 'polaridade']][df_aux['polaridade']=='negativo']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'Negativo', ax=ax, color = 'red')\n",
    "\n",
    "ax.legend(['Positivo', 'Neutro', 'Negativo'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.select_dtypes(include=['float64']).columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,6))\n",
    "#sns.heatmap(df[df.select_dtypes(include=['float64']).columns].corr(), annot=True)\n",
    "#plt.title('data correlations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anotação Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_manual'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_manual'].value_counts().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "df[['data2', 'sent_manual']][df['sent_manual']=='C']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth=5, y = 'C', ax=ax, color = 'red')\n",
    "\n",
    "df[['data2', 'sent_manual']][df['sent_manual']=='N']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth=5, y = 'N', ax=ax, color = 'black')\n",
    "\n",
    "df[['data2', 'sent_manual']][df['sent_manual']=='E']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot( linewidth=5, y = 'N', ax=ax, color = 'blue')\n",
    "\n",
    "ax.legend(['C', 'N', 'E'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### críticas por usuário (anotadas manualmente) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sent_manual']=='C']['usuario'].value_counts().plot.bar(title='críticas por usuários')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['sent_manual2'].value_counts().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "df5[['data2', 'sent_manual2']][df5['sent_manual2']=='C']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'C', ax=ax, color = 'red')\n",
    "\n",
    "df5[['data2', 'sent_manual2']][df5['sent_manual2']=='X']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'X', ax=ax, color = 'blue')\n",
    "\n",
    "df5[['data2', 'sent_manual2']][df5['sent_manual2']=='N']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'N', ax=ax, color = 'green')\n",
    "\n",
    "df5[['data2', 'sent_manual2']][df5['sent_manual2']=='A']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = 'A', ax=ax, color = 'magenta')\n",
    "\n",
    "\n",
    "ax.legend(['C', 'X', 'N', 'A'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vocab = pickle.load(open(\"MLP-binary-vocab\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTM-BINÁRIA\n",
    "# cria um objeto contador binário\n",
    "binary_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',binary=True, vocabulary=binary_vocab)\n",
    "binary_vect.fit(df['tweet_limpo']) # treina o objeto nos textos processados\n",
    "#print(binary_vect.get_feature_names())\n",
    "#print(len(binary_vect.get_feature_names()),\" tokens\")\n",
    "#Transforma os documentos na matriz documento termo binária.\n",
    "xvalid_binary =  binary_vect.transform(df['tweet_limpo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "classifier = joblib.load(open(\"MLP-binary\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(xvalid_binary)\n",
    "Counter(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MLP_binary'] = predictions\n",
    "df['MLP_binary'] = df['MLP_binary'].apply(lambda x: \"N\" if x==1 else \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MLP_binary'].value_counts().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "df[['data2', 'MLP_binary']][df['MLP_binary']=='C']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth=5, y = 'C', ax=ax, color = 'red')\n",
    "\n",
    "df[['data2', 'MLP_binary']][df['MLP_binary']=='N']['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth=5, y = 'N', ax=ax, color = 'black')\n",
    "\n",
    "ax.legend(['C', 'N'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_aux = pd.DataFrame({\n",
    "    \"N\":df[['data2', 'MLP_binary']][(df['MLP_binary']=='N') & (df['data2']>datetime.date(2020,3,3))]['data2'].value_counts().sort_index().asfreq('D').fillna(0).values,\n",
    "    \"C\":df[['data2', 'MLP_binary']][df['MLP_binary']=='C']['data2'].value_counts().sort_index().asfreq('D').fillna(0).values},\n",
    "    index = df[['data2', 'MLP_binary']][df['MLP_binary']=='C']['data2'].value_counts().sort_index().asfreq('D').fillna(0).index\n",
    ")\n",
    "df_aux['razao'] = df_aux[\"C\"]/(df_aux[\"N\"]+0.01)*100\n",
    "df_aux.apply(lambda x: np.log(x + 0.01)).razao.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 MODELAGEM DE TÓPICOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Analysis (LDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tweet_limpo'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(corpus)):\n",
    "    corpus[i]=corpus[i].lower()\n",
    "    corpus[i] = re.sub('[0-9]+', '', corpus[i]) #remove numbers\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i]) #remove punctuation\n",
    "    corpus[i] = re.sub('\\n','',corpus[i]) #remove \\n - newline\n",
    "    #corrige expressões comuuns\n",
    "    corpus[i] = re.sub('paulo guedes','paulo_guedes',corpus[i]) \n",
    "    corpus[i] = re.sub('política monetária','política_monetária',corpus[i]) \n",
    "    corpus[i] = re.sub('roberto campos neto','roberto_campos_neto',corpus[i]) \n",
    "    corpus[i] = re.sub('reservas internacionais','reservas_internacionais',corpus[i]) \n",
    "    corpus[i] = re.sub('produto interno bruto','pib',corpus[i]) \n",
    "    corpus[i] = re.sub('instituição financeira','if',corpus[i]) \n",
    "    corpus[i] = re.sub('instituições financeiras','if',corpus[i]) \n",
    "    corpus[i] = re.sub('operação compromissada','operação_compromissada',corpus[i]) \n",
    "    corpus[i] = re.sub('operações compromissadas','operação_compromissada',corpus[i]) \n",
    "    corpus[i] = re.sub('relatório trimestral de inflação','rti',corpus[i]) \n",
    "    corpus[i] = re.sub('orçamento de guerra','orçamento_de_guerra',corpus[i]) \n",
    "    corpus[i] = re.sub('bi ','bilhões ',corpus[i]) \n",
    "    corpus[i] = re.sub('tri ','trilhões ',corpus[i]) \n",
    "    corpus[i] = re.sub('reunião do copom','copom',corpus[i]) \n",
    "    corpus[i] = re.sub('cheque especial','cheque_especial',corpus[i]) \n",
    "    corpus[i] = re.sub('conselho monetário nacional','cmn',corpus[i]) \n",
    "    corpus[i] = re.sub('relatório focus','focus',corpus[i]) \n",
    "    corpus[i] = re.sub('reclamações','reclamação',corpus[i]) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega modelo pré-treinado para processar textos em português. Desabilita duas funções que não vamos usar\n",
    "#nlp = spacy.load('pt_core_news_sm', disable=['parser', 'ner'])\n",
    "\n",
    "#for i in range(0,len(corpus)): # varre a lista de textos\n",
    "#    doc = nlp(corpus[i]) # executa um processamento de texto\n",
    "#    corpus[i]=\" \".join([token.lemma_ for token in doc]) # substitui o texto anterior por um texto contendo os lemas extraídos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mystopwords = Mystopwords + ['gt','bps','dias','tempo','cenário','evitar','presidente','acesse','site','texto','telefone','precisa','registre','ouvindo']\n",
    "\n",
    "for i in range(0,len(corpus)): # varre a lista de textos\n",
    "    words=corpus[i].split(\" \") # separa o texto em palavras\n",
    "    words_new = [w for w in words if w not in Mystopwords] #remove as stop words\n",
    "    corpus[i] = ' '.join(words_new) # concantena as palavras novamente\n",
    "\n",
    "corpus_NMF = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(corpus)):\n",
    "    corpus[idx] = [word for word in corpus[idx].split(' ') if word not in ['']];\n",
    "\n",
    "#corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_topics = 5\n",
    "id2word = gensim.corpora.Dictionary(corpus);\n",
    "corpus_idx = [id2word.doc2bow(text) for text in corpus];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=corpus_idx, id2word=id2word, num_topics=Nr_topics,passes=4,iterations=400)\n",
    "#WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 10);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(lda, Nr_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics=[]\n",
    "for doc in corpus_idx:\n",
    "    lista_tuplas = lda.get_document_topics(doc)\n",
    "    top_aux = 0\n",
    "    prob_aux = 0\n",
    "    for top, prob in lista_tuplas:\n",
    "        if prob>prob_aux:\n",
    "            top_aux = top\n",
    "    doc_topics.append(top_aux)\n",
    "df['lda_topico'] = doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lda_topico'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Nr_topics):\n",
    "    print(\"Tópico:\", i)\n",
    "    print(df[['usuario','lda_topico']][df['lda_topico']==i]['usuario'].value_counts().head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyLDAvis.gensim.prepare(lda, corpus_idx, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 3),\n",
    "                             #stop_words = Mystopwords, #NÃO TIRAR STOP WORDS, POIS JÁ FORAM RETIRADOS\n",
    "                             #max_df = 10000, \n",
    "                             #min_df=10,\n",
    "                             #max_features=10000\n",
    "                            )\n",
    "count_vect.fit(corpus_NMF) # treina o objeto nos textos processados\n",
    "count_vect_dtm = count_vect.transform(corpus_NMF)\n",
    "count_vect_dtm.shape\n",
    "vocab = np.array(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_topics = 5\n",
    "\n",
    "clf = decomposition.NMF(n_components=Nr_topics, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(count_vect_dtm)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=5\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' - '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, feat_names, num_topics):\n",
    "    '''\n",
    "    Model = modelo treinado na função fit do NMF\n",
    "    feat_names = lista de palavras obtidas de vectorizer.get_feature_names() (vocabulário)\n",
    "    Nr_topics = número de tópicos\n",
    "    '''\n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    \n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(clf, vocab, Nr_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W1.shape\n",
    "df['nmf_topico'] = W1.argmax(axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nmf_topico'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Nr_topics):\n",
    "    print(\"Tópico:\",i+1)\n",
    "    print(show_topics(H1)[i])\n",
    "    print(df[['usuario','nmf_topico']][df['nmf_topico']==i+1]['usuario'].value_counts().head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "df[['data2', 'nmf_topico']][df['nmf_topico']==1]['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = '1', ax=ax, color = 'red')\n",
    "\n",
    "df[['data2', 'nmf_topico']][df['nmf_topico']==2]['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = '2', ax=ax, color = 'blue')\n",
    "\n",
    "df[['data2', 'nmf_topico']][df['nmf_topico']==3]['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = '3', ax=ax, color = 'green')\n",
    "\n",
    "df[['data2', 'nmf_topico']][df['nmf_topico']==4]['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = '4', ax=ax, color = 'magenta')\n",
    "\n",
    "df[['data2', 'nmf_topico']][df['nmf_topico']==5]['data2'].value_counts().sort_index().asfreq('D').fillna(0).\\\n",
    "plot(xlim=('2020-03-01',np.max(df['data'])), linewidth=5, y = '5', ax=ax, color = 'black')\n",
    "\n",
    "ax.legend(['1', '2', '3', '4','5'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = tp.HLDAModel.load('models/hLDA.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(mdl.k):\n",
    "    if not mdl.is_live_topic(k): continue\n",
    "    print('Topic #{}'.format(k))\n",
    "    print('Level', mdl.level(k))\n",
    "    #print('Alive',mdl.is_live_topic(k))\n",
    "    print('Nr Docs:', mdl.num_docs_of_topic(k))\n",
    "    print('Parent Topics:')\n",
    "    print(mdl.parent_topic(k))\n",
    "    print('Children Topics:')\n",
    "    print(mdl.children_topics(k))\n",
    "    for word, prob in mdl.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = pd.read_pickle('dados\\\\df_hlda_topics.pkl')\n",
    "df = df.merge(df_aux, how='left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hlda_topics'].value_counts().head(20).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDE DO HLDA COM NÓS SENDO OS TÓPICOS E OS VÉRTICES SENDO AS RELAÇÕES DE HIERARQUIA ENTRE OS TÓPICOS\n",
    "Ghlda = nx.Graph()   \n",
    "for k in range(mdl.k):\n",
    "    if not mdl.is_live_topic(k): continue\n",
    "    lista_top = mdl.children_topics(0).tolist()\n",
    "    if len(lista_top)>0:\n",
    "        for l in lista_top:\n",
    "            Ghlda.add_edge(k, l)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(Ghlda, with_labels=True,font_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 CLUSTERING\n",
    "\n",
    "Baseado em https://github.com/twitterdev/do_more_with_twitter_data/blob/master/examples/clustering_users/clustering-users.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tweets = TfidfVectorizer(stop_words=Mystopwords+\\\n",
    "                      ['of','rio','and','are','my','co','own','https','the','t','ex','at','rt','nunca','tudo',\\\n",
    "                      'alvaro','si'],\n",
    "                             #max_df=1.0,\n",
    "                             min_df=20,\n",
    "                             #max_features=4000,\n",
    "                      \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_matrix = vec_tweets.fit_transform(unique_tweets)\n",
    "tweet_matrix = vec_tweets.fit_transform(df['tweet_limpo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbs_tweets = hdbscan.HDBSCAN(min_cluster_size=25,\n",
    "                               prediction_data=True,\n",
    "                               core_dist_n_jobs=-1,\n",
    "                               memory='data')\n",
    "hdbs_tweets.fit(tweet_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the population sizes \n",
    "label_counts = Counter(hdbs_tweets.labels_)\n",
    "xs, ys = [], []\n",
    "for k,v in label_counts.items():\n",
    "    xs.append(k)\n",
    "    ys.append(v)\n",
    "\n",
    "# draw the chart\n",
    "plt.bar(xs, ys)\n",
    "\n",
    "plt.xticks(range(-1, len(label_counts)))\n",
    "plt.ylabel('population')\n",
    "plt.xlabel('cluster label')\n",
    "plt.title('population sizes ({} clusters found by hdbscan)'.format(len(label_counts) - 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Número de clusters:\", hdbs_tweets.labels_.max()+1 )\n",
    "print(\"(cluster, qtd)\")\n",
    "([(xs[i], ys[i]) for i in range(len(xs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strongest_features(hdbs_tweets, vec_tweets, tweet_matrix, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = hdbs_tweets.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongest_features_per_cluster(cluster, model, vectorizer, matrix, topk=10):\n",
    "    \"\"\"\n",
    "    Helper function to display a simple text representation of the top-k most \n",
    "    important features in our fit model and vectorizer.\n",
    "    \n",
    "    cluster: cluster number\n",
    "    matrix: matriz obtida da função vec.fit_transform\n",
    "    vectorizer: sklearn vectorizer\n",
    "    topk: k numbers of words to get per cluster\n",
    "    \n",
    "    \"\"\"\n",
    "    features = vectorizer.get_feature_names()\n",
    "    # ignore noise labels\n",
    "    relevant_labels = [ x for x in set(model.labels_) if x >= 0 ]\n",
    "    this_label = cluster\n",
    "    matching_rows = np.where(hdbs.labels_ == this_label)[0]\n",
    "    coeff_sums = np.sum(matrix[matching_rows], axis=0).A1\n",
    "    sorted_coeff_idxs = np.argsort(coeff_sums)[::-1]\n",
    "    print('Cluster {}: '.format(this_label), end='')\n",
    "    for idx in sorted_coeff_idxs[:topk]:\n",
    "        print('{} '.format(features[idx]), end='')\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(-1,hdbs_tweets.labels_.max()+1):\n",
    "    #print(\"Cluster:\",i)\n",
    "    print(strongest_features_per_cluster(i,hdbs_tweets,vec_tweets,tweet_matrix,10))\n",
    "    print(df[['usuario','cluster']][df['cluster']==i]['usuario'].value_counts().head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 NER \n",
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm', disable=['tagger', 'parser', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NER(text):\n",
    "    doc = nlp(text)\n",
    "    return [(e.text, e.label_) for e in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df['tweet'].apply(get_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tweet','entities']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_entities = df['entities'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "len(lista_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(lista_entities).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_entities2 = [ent for ent,tipo in lista_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(lista_entities2).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ANÁLISE DE REDES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de redes - Grafos  \n",
    "\n",
    "https://github.com/ericmjl/Network-Analysis-Made-Simple/\n",
    "\n",
    "https://github.com/ericmjl/nxviz/tree/master/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exists(node1, node2, G):\n",
    "    \"\"\"\n",
    "    This function checks whether a path exists between two nodes (node1, \n",
    "    node2) in graph G.\n",
    "    \n",
    "    Special thanks to @ghirlekar for suggesting that we keep track of the \n",
    "    \"visited nodes\" to prevent infinite loops from happening. This also \n",
    "    removes the need to remove nodes from queue.\n",
    "    \n",
    "    Reference: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/3\n",
    "    \n",
    "    With thanks to @joshporter1 for the second bug fix. Originally there was \n",
    "    an extraneous \"if\" statement that guaranteed that the \"False\" case would \n",
    "    never be returned - because queue never changes in shape. Discovered at \n",
    "    PyCon 2017.\n",
    "    \n",
    "    With thanks to @chendaniely for pointing out the extraneous \"break\".\n",
    "    \n",
    "    If you would like to see @dgerlanc's implementation, see \n",
    "    https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/76\n",
    "    \"\"\"\n",
    "    visited_nodes = set()\n",
    "    queue = [node1]\n",
    "    \n",
    "    for node in queue:\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if node2 in neighbors:\n",
    "            print('Path exists between nodes {0} and {1}'.format(node1, node2))\n",
    "            return True\n",
    "        else:\n",
    "            visited_nodes.add(node)\n",
    "            queue.extend([n for n in neighbors if n not in visited_nodes])\n",
    "    \n",
    "    print('Path does not exist between nodes {0} and {1}'.format(node1, node2))\n",
    "    return False\n",
    "\n",
    "def extract_path_edges(G, source, target):\n",
    "    # Check to make sure that a path does exists between source and target.\n",
    "    if nx.has_path(G, source, target):\n",
    "        nodes = nx.shortest_path(G, source, target)\n",
    "        newG = G.subgraph(nodes)\n",
    "        return newG\n",
    "\n",
    "    else:\n",
    "        raise Exception('Path does not exist between nodes {0} and {1}.'.format(source, target))\n",
    "\n",
    "def extract_neighbor_edges(G, node):\n",
    "    neighbors = list(G.neighbors(node))\n",
    "    newG = nx.Graph()\n",
    "    \n",
    "    for n1, n2 in G.edges():\n",
    "        if (n1 == node and n2 in neighbors) or (n1 in neighbors and n2 == node):\n",
    "            newG.add_edge(n1, n2)\n",
    "            \n",
    "    return newG\n",
    "\n",
    "\n",
    "def extract_neighbor_edges2(G, node):\n",
    "    neighbors = G.neighbors(node)\n",
    "    newG = nx.Graph()\n",
    "    \n",
    "    for neighbor in neighbors:\n",
    "        if (node, neighbor) in G.edges() or (neighbor, node) in G.edges():\n",
    "            newG.add_edge(node, neighbor)\n",
    "\n",
    "    return newG\n",
    "\n",
    "def get_triangles(G, node):\n",
    "    neighbors1 = set(G.neighbors(node))\n",
    "    triangle_nodes = set()\n",
    "    triangle_nodes.add(node)\n",
    "    \"\"\"\n",
    "    Fill in the rest of the code below.\n",
    "    \"\"\"\n",
    "    for nbr1, nbr2 in itertools.combinations(neighbors1, 2):\n",
    "        if G.has_edge(nbr1, nbr2):\n",
    "            triangle_nodes.add(nbr1)\n",
    "            triangle_nodes.add(nbr2)\n",
    "    return triangle_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g= pd.read_pickle('dados\\\\df_tweet_grafo.pkl')\n",
    "df_g.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = df_g[df_g['usuario1'].isin(lista_usuarios2)][df_g['usuario2'].isin(lista_usuarios2)]\\\n",
    "[df_g['data']>\"2020-02-29\"]\n",
    "df_g.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g['tipo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = nx.from_pandas_edgelist(df_g, 'usuario1', 'usuario2', ['likes', 'retweets','tipo'], create_using=nx.DiGraph())\n",
    "nx.write_gml(GT,\"dados\\\\grafo_geral.gml\")\n",
    "dic = {}\n",
    "for user in df['usuario'].unique():\n",
    "    dic[user] = df[df['usuario']==user]['tipo_usuario'].values[0]\n",
    "nx.set_node_attributes(GT, values=dic, name='tipo_usuario')\n",
    "print(nx.info(GT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ANÁLISE DE TOPOLOGIA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_directed(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_strongly_connected(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_weakly_connected(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_weighted(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.is_aperiodic(GT)\n",
    "#muitos warnings\n",
    "#False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.classes.density(GT)\n",
    "#The density is 0 for a graph without edges and 1 for a complete graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.reciprocity(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.algorithms.cluster.transitivity(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_pearson_correlation_coefficient(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(nx.algorithms.community.greedy_modularity_communities(GT))\n",
    "#IndexError: list index out of range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"isolates: \", nx.algorithms.isolate.number_of_isolates(GT))\n",
    "if nx.algorithms.isolate.number_of_isolates(GT)>0:\n",
    "    list(nx.isolates(GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.networkx.algorithms.connectivity.connectivity.node_connectivity(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(GT, with_labels=True,font_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ANÁLISE DE CENTRALIDADE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medidas de centralidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mariores Graus.\n",
    "sorted(GT.degree(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality\n",
    "sorted(nx.degree_centrality(GT).items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality\n",
    "sorted(nx.in_degree_centrality(GT).items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality\n",
    "sorted(nx.out_degree_centrality(GT).items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLOSENESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness\n",
    "closeness_centrality = nx.closeness_centrality(GT) \n",
    "sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BETWEENESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweeness\n",
    "betweeness_centrality = nx.betweenness_centrality(GT)\n",
    "sorted(betweeness_centrality.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CENTRALIDADE DE AUTOVETOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvector_centrality\n",
    "eigenvector_centrality = nx.eigenvector_centrality_numpy(GT)\n",
    "\n",
    "# normaliza o eigenvector_centrality\n",
    "max_value = max(eigenvector_centrality.items(), key=lambda x: x[1])\n",
    "\n",
    "ec_scaled = {}\n",
    "for k in eigenvector_centrality.keys():\n",
    "    ec_scaled[k] = eigenvector_centrality[k] / max_value[1]\n",
    "\n",
    "# Scaled by the most central character (karev)\n",
    "sorted(ec_scaled.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAGERANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.pagerank(GT, alpha=0.9)\n",
    "sorted(nx.pagerank(GT, alpha=0.9).items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KATZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# katz_centrality\n",
    "katz_centrality = nx.katz_centrality_numpy(GT)\n",
    "\n",
    "# normaliza o eigenvector_centrality\n",
    "max_value = max(katz_centrality.items(), key=lambda x: x[1])\n",
    "\n",
    "ec_scaled = {}\n",
    "for k in katz_centrality.keys():\n",
    "    ec_scaled[k] = katz_centrality[k] / max_value[1]\n",
    "\n",
    "# Scaled by the most central character (karev)\n",
    "sorted(ec_scaled.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ANÁLISE DE COMUNIDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEMORA\n",
    "gn_comm = girvan_newman(GT)\n",
    "first_iteration_comm = tuple(sorted(c) for c in next(gn_comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_color_map(G, attribute, seaborn_palette=\"colorblind\"):\n",
    "    \"\"\"Return a list of hex color mappings for node attributes\"\"\"\n",
    "    attributes = [G.nodes[label][attribute] for label in G.nodes()]\n",
    "\n",
    "    # get the set of possible attributes\n",
    "    attributes_unique = list(set(attributes))\n",
    "    num_values = len(attributes_unique)\n",
    "\n",
    "    # generate color palette from seaborn\n",
    "    palette = sns.color_palette(seaborn_palette, num_values).as_hex()\n",
    "\n",
    "    # create a mapping of attribute to color\n",
    "    color_map = dict(zip(attributes_unique, palette))\n",
    "\n",
    "    # map the attribute for each node to the color it represents\n",
    "    node_colors = [color_map[attribute] for attribute in attributes]\n",
    "\n",
    "    return node_colors, color_map, palette\n",
    "\n",
    "def map_communities(G, communities):\n",
    "    \"\"\"Return a mapping of community membership from a community set tuple\"\"\"\n",
    "\n",
    "    community_map = {}\n",
    "    for node in G.nodes():\n",
    "        for i, comm in enumerate(communities):\n",
    "            if node in comm:\n",
    "                community_map[node] = i\n",
    "        if community_map.get(node, None) is None:\n",
    "            community_map[node] = None\n",
    "    return community_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_map = map_communities(GT, first_iteration_comm)\n",
    "nx.set_node_attributes(GT, name='community', values=community_map)\n",
    "node_colors, color_map, palette = create_color_map(GT, 'community')\n",
    "print(\"Número de comunidades: \", max(community_map.values())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(GT, node_color=node_colors, with_labels=True,font_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(GT,node_color=node_colors, with_labels=True,font_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(community_map.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_community_girvan_newman(user):\n",
    "    if user in community_map.keys():\n",
    "        return community_map[user] + 1\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "df['girvan_newman'] = df['usuario'].apply(retorna_community_girvan_newman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = (\n",
    "    node\n",
    "    for node, data\n",
    "    in GT.nodes(data=True)\n",
    "    if data.get(\"community\") == 1\n",
    ")\n",
    "nx.draw(GT.subgraph(nodes),with_labels=True,font_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecção de comunidades 2\n",
    "\n",
    "Clauset-Newman-Moore greedy modularity maximization\n",
    "\n",
    "É necessário converter o gráfico para indireto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = list(greedy_modularity_communities(GT.to_undirected()))\n",
    "print(\"Nr de comunidades:\", len(c),\"\\n\")\n",
    "dict_modularity = {}\n",
    "for i, l in enumerate(c, start = 1):\n",
    "    print(\"comunidade \",i,\" \",l,\"\\n\")\n",
    "    for u in l:\n",
    "        dict_modularity[u] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_community_modularity(user):\n",
    "    if user in dict_modularity.keys():\n",
    "        return dict_modularity[user]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['modularity'] = df['usuario'].apply(retorna_community_modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5 maiores comunidades\")\n",
    "c = Counter(list(dict_modularity.values())).most_common(5)\n",
    "c[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    j = c[i][0]\n",
    "    print(\"Comunidade:\", j,\"\\nTóp - Qtd\")\n",
    "    print(df[['modularity','nmf_topico']][df['modularity']==j]['nmf_topico'].value_counts().head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecção de comunidades 3\n",
    "(conversão para grafo indireto)\n",
    "\n",
    "Fonte: https://www.analyticsvidhya.com/blog/2020/04/community-detection-graphs-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_to_remove(graph):\n",
    "    G_dict = nx.edge_betweenness_centrality(graph)\n",
    "    edge = ()\n",
    "\n",
    "    # extract the edge with highest edge betweenness centrality score\n",
    "    for key, value in sorted(G_dict.items(), key=lambda item: item[1], reverse = True):\n",
    "        edge = key\n",
    "        break\n",
    "\n",
    "    return edge\n",
    "\n",
    "def girvan_newman(graph):\n",
    "\t# find number of connected components\n",
    "\tsg = nx.connected_components(graph)\n",
    "\tsg_count = nx.number_connected_components(graph)\n",
    "\n",
    "\twhile(sg_count == 1):\n",
    "\t\tgraph.remove_edge(edge_to_remove(graph)[0], edge_to_remove(graph)[1])\n",
    "\t\tsg = nx.connected_components(graph)\n",
    "\t\tsg_count = nx.number_connected_components(graph)\n",
    "\n",
    "\treturn sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find communities in the graph\n",
    "c = girvan_newman(GT.to_undirected(reciprocal=True).copy())\n",
    "\n",
    "# find the nodes forming the communities\n",
    "node_groups = []\n",
    "\n",
    "for i in c:\n",
    "    node_groups.append(list(i))\n",
    "    \n",
    "node_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DETECÇÃO DE EVENTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/davidzchen-ut/TwitterEvents/blob/master/TwitterEvents.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['data']>\"2020-03-01\"][['data','data2','tweet','tweet_limpo']].copy()\n",
    "df2['Datetime']= pd.to_datetime(df2['data2'].apply(str)+' '+df2['data'].apply(lambda x: x.hour).apply(str)+':00')\n",
    "\n",
    "tableFlag=[]\n",
    "\n",
    "count = 0\n",
    "# iterating through clusters and finding hourly count of tweets\n",
    "\n",
    "dfchange = df2\n",
    "ts = dfchange.set_index('Datetime')\n",
    "vc = ts.groupby('Datetime').count()\n",
    "col = ['tweet']\n",
    "vc2 = vc[col]\n",
    "##calculating threshold to flag hours with more tweets\n",
    "th = vc2['tweet'].mean()+2*vc2['tweet'].std()\n",
    "#iterating through each row to find flags\n",
    "for index, row in vc2.iterrows():\n",
    "    if (row[\"tweet\"]>th):\n",
    "        tableFlag.append([index])\n",
    "        count+=1\n",
    "\n",
    "print(count)\n",
    "#tableFlag\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing the threshold vs hourly tweet occurences\n",
    "#dfchange = df2\n",
    "#ts = dfchange.set_index('Datetime')\n",
    "#vc = ts.groupby('Datetime').count()\n",
    "#col = ['tweet']\n",
    "#vc2 = vc[col]\n",
    "vc3 = vc2.copy()\n",
    "#th = vc2['tweet'].mean()+2*vc2['tweet'].std()\n",
    "#for index, row in vc2.iterrows():\n",
    "#    if (row[\"tweet\"]>th):\n",
    "#        tableFlag.append([index])\n",
    "#        count+=1\n",
    "\n",
    "vc4 = vc2.copy()\n",
    "vc4.rename(columns={'tweet':'Hourly'},inplace=True)\n",
    "vc3['tweet'] = th\n",
    "vc3.rename(columns={'tweet':'Threshold'},inplace=True)\n",
    "\n",
    "ax = vc2.plot()\n",
    "\n",
    "#ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "vc3.plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve tweets associated with flagged timestamp\n",
    "df2['event_id'] = -1\n",
    "for index, array in enumerate(tableFlag):\n",
    "    #cluster = array[0]\n",
    "    timestamp = array[0]\n",
    "    print(array[0])\n",
    "    df2['event_id'] = (np.where((df2['Datetime'] == timestamp), index, df2[\"event_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = df2\n",
    "\n",
    "nr_spikes = np.max(events['event_id'])+1\n",
    "# Find all tweets that created each spike (950 spikes in our case)\n",
    "tweetsPerEvent = []\n",
    "for i in range(0, nr_spikes):\n",
    "    tweetsPerEvent.append(events[events['event_id'] == i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10\n",
    "#stemmer = SnowballStemmer(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get TF-IDF and Similarity Matrices\n",
    "def getTFIDFAndSim(tweets):\n",
    "    vect = TfidfVectorizer(max_df=0.8, max_features=200000, stop_words=Mystopwords, use_idf=True, ngram_range=(1,3))\n",
    "    tfidf = vect.fit_transform(tweets)\n",
    "    sim = (tfidf * tfidf.T).A\n",
    "    return tfidf, sim\n",
    "\n",
    "#Find Clusters of Similar Tweets\n",
    "def findSimilarTweets(tweets, tfidf):\n",
    "    clustering = DBSCAN(eps=1.2, min_samples=2).fit(tfidf)\n",
    "    #clustering = hdbscan.HDBSCAN(min_cluster_size=2).fit(tfidf)\n",
    "\n",
    "    clusters = clustering.labels_.tolist()\n",
    "    temp = clusters.copy()\n",
    "    temp.remove\n",
    "\n",
    "    addedCluster = tweets.copy()\n",
    "    addedCluster['Cluster'] = clusters\n",
    "\n",
    "    #pd.options.display.max_colwidth = 100\n",
    "\n",
    "    cls = []\n",
    "    for i in range(0, num_clusters):\n",
    "        tweetsInCluster = addedCluster[addedCluster['Cluster'] == i]\n",
    "        cls.append(tweetsInCluster)\n",
    "        \n",
    "        if(tweetsInCluster.shape[0] > 1): #há tweets repetidos, por isso > 2\n",
    "            #print(\"Cluster\", i, \":\", tweetsInCluster.shape[0])\n",
    "            print(\"Evento:\", tweetsInCluster['event_id'].to_list()[0])\n",
    "            print(\"Data-hora:\", tweetsInCluster['Datetime'].to_list()[0])\n",
    "            for txt in tweetsInCluster['tweet'].to_list()[:10]:\n",
    "                print(txt)\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "    #return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "#df = pd.DataFrame();\n",
    "    \n",
    "# For each spike, clean the tweets, stem and tokenize the tweets, and find the TF-IDF vectors for the tweets. Then cluster\n",
    "# on the vectors to find similar tweets. If clusters are found, then take the largest cluster to find the tweets that define\n",
    "# the event.\n",
    "for i in range(0, len(tweetsPerEvent)):\n",
    "    if(tweetsPerEvent[i].shape[0] < th):\n",
    "        continue\n",
    "    \n",
    "    count += 1\n",
    "    #tweets = cleanData(tweetsPerEvent[i])\n",
    "    #tweetsContent = tweets.copy()['tweet']\n",
    "    tweets = tweetsPerEvent[i]\n",
    "    tweetsContent = tweets.copy()['tweet_limpo']\n",
    "    tfidf, sim = getTFIDFAndSim(tweetsContent)\n",
    "    #totalvocab_stemmed, totalvocab_tokenized = stemAndTokenize(tweetsContent)\n",
    "    #print(\"Event\", i, \":\")\n",
    "    #print(tweetsPerEvent[i][['data','tweet_limpo']])\n",
    "    #if(findSimilarTweets(tweets, tfidf)):\n",
    "    #    df = pd.concat([df, tweetsPerEvent[i]])\n",
    "    findSimilarTweets(tweets, tfidf)\n",
    "    print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TENDÊNCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = df[['data','tweet_limpo']].copy()\n",
    "df_test.columns = ['data','tweet']\n",
    "df_test['data'] = df_test['data'].apply(lambda x: x.date())\n",
    "\n",
    "df_test2 = df_test.groupby(['data'])['tweet'].apply(lambda x: ' '.join(x)).reset_index().sort_values(by='data')\n",
    "df_test2.set_index('data', inplace=True)\n",
    "df_test2['tweet'] = df_test2['tweet'].apply(lambda x: x.lower())\n",
    "df_test2['tweet'] = df_test2['tweet'].apply(lambda x: re.sub('\\n', ' ', x))\n",
    "\n",
    "#df_test2 = df_test2.tweet.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "i = df_test2.index[0]\n",
    "df_aux = df_test2.loc[[i]]['tweet'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "df_aux.columns = ['qtd']\n",
    "df_aux['data'] = df_aux['qtd'].apply(lambda x: i)\n",
    "df_aux = df_aux.reset_index()\n",
    "df_aux.columns = ['palavra', 'qtd', 'data']\n",
    "\n",
    "for i in df_test2.index[1:]:\n",
    "    df_aux2 = df_test2.loc[[i]]['tweet'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "    df_aux2.columns = ['qtd']\n",
    "    df_aux2['data'] = df_aux2['qtd'].apply(lambda x: i)\n",
    "    df_aux2 = df_aux2.reset_index()\n",
    "    df_aux2.columns = ['palavra', 'qtd', 'data']\n",
    "    df_aux = pd.concat([df_aux, df_aux2])\n",
    "    \n",
    "\n",
    "\n",
    "#BIGRAMAS\n",
    "\n",
    "i = df_test2.index[0]\n",
    "text =  np.array2string( df_test2.loc[[i],['tweet']].values)\n",
    "text = re.sub('\\[',' ',text)\n",
    "text = re.sub('\\]',' ',text)\n",
    "bigrams = [a+' '+b for l in [text] for a,b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "bigrams = Counter(bigrams)\n",
    "df_aux2 = pd.DataFrame.from_dict(bigrams, orient='index').reset_index()\n",
    "df_aux2.columns = ['palavra','qtd']\n",
    "df_aux2 = df_aux2[ df_aux2['palavra']  != ' ']\n",
    "df_aux2['data'] = df_aux2['qtd'].apply(lambda x: i)\n",
    "df_aux = pd.concat([df_aux, df_aux2])\n",
    "\n",
    "for i in df_test2.index[1:]:\n",
    "    text =  np.array2string( df_test2.loc[[i],['tweet']].values)\n",
    "    text = re.sub('\\[',' ',text)\n",
    "    text = re.sub('\\]',' ',text)\n",
    "    bigrams = [a+' '+b for l in [text] for a,b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "    bigrams = Counter(bigrams)\n",
    "    df_aux2 = pd.DataFrame.from_dict(bigrams, orient='index').reset_index()\n",
    "    df_aux2.columns = ['palavra','qtd']\n",
    "    df_aux2 = df_aux2[ df_aux2['palavra']  != ' ']\n",
    "    df_aux2['data'] = df_aux2['qtd'].apply(lambda x: i)\n",
    "    df_aux = pd.concat([df_aux, df_aux2])\n",
    "\n",
    "#TRIGRAMAS\n",
    "\n",
    "i = df_test2.index[0]\n",
    "text =  np.array2string( df_test2.loc[[i],['tweet']].values)\n",
    "text = re.sub('\\[',' ',text)\n",
    "text = re.sub('\\]',' ',text)\n",
    "trigrams = [a+' '+b+' '+c for l in [text] for a,b,c in zip(l.split(\" \")[:-2], l.split(\" \")[1:-1], l.split(\" \")[2:])]\n",
    "trigrams = Counter(trigrams)\n",
    "df_aux2 = pd.DataFrame.from_dict(trigrams, orient='index').reset_index()\n",
    "df_aux2.columns = ['palavra','qtd']\n",
    "df_aux2 = df_aux2[ df_aux2['palavra']  != ' ']\n",
    "df_aux2['data'] = df_aux2['qtd'].apply(lambda x: i)\n",
    "df_aux = pd.concat([df_aux, df_aux2])\n",
    "\n",
    "for i in df_test2.index[1:]:\n",
    "    text =  np.array2string( df_test2.loc[[i],['tweet']].values)\n",
    "    text = re.sub('\\[',' ',text)\n",
    "    text = re.sub('\\]',' ',text)\n",
    "    trigrams = [a+' '+b+' '+c for l in [text] for a,b,c in zip(l.split(\" \")[:-2], l.split(\" \")[1:-1], l.split(\" \")[2:])]\n",
    "    trigrams = Counter(trigrams)\n",
    "    df_aux2 = pd.DataFrame.from_dict(trigrams, orient='index').reset_index()\n",
    "    df_aux2.columns = ['palavra','qtd']\n",
    "    df_aux2 = df_aux2[ df_aux2['palavra']  != ' ']\n",
    "    df_aux2['data'] = df_aux2['qtd'].apply(lambda x: i)\n",
    "    df_aux = pd.concat([df_aux, df_aux2])\n",
    "\n",
    "df_ngrams = df_aux.copy()\n",
    "\n",
    "del df_test, df_test2, df_aux, df_aux2#, bigrams, trigrams\n",
    "\n",
    "#transformação de qtd em indice = nr_expressoes/nr_tweets\n",
    "\n",
    "df_aux = df['data'].apply(lambda x: x.date()).value_counts().to_frame().reset_index()\n",
    "df_aux.columns = ['data','nr']\n",
    "df_ngrams = df_ngrams.merge(df_aux , how='left', on='data')\n",
    "df_ngrams['indice'] = df_ngrams['qtd']/df_ngrams['nr']\n",
    "del df_aux\n",
    "\n",
    "df_ngrams = df_ngrams[['palavra','data','indice']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'] == 'copom'].set_index('data').sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'] == 'paulo guedes'].set_index('data').sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth = 10)# pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'] == 'roberto campos'].set_index('data').sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth = 10) #rcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'] == 'reservas'].set_index('data').sort_index().asfreq('D').fillna(0).\\\n",
    "plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['bcb','banco central','bacen']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['cmn','conselho monetário nacional']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['política monetária','politica monetaria']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['coronavirus','covid-19','coronavírus','covid19','corona virus','corona vírus','pandemia','covid','ncov19','ncov2019']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['dólar','dólares','dolar','dolares','câmbio','política cambial']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['ipca','inflação','deflação']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['juros','selic','copom','di']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join([\"incerteza\",\"incertezas\",\"incerto\",\"incerta\",\"dúvida\",\"dúvidas\",\"hesitação\", \n",
    "                                                      \"imprecisão\", \"indecisão\", \"indefinição\", \"indeterminação\", \"insegurança\", \n",
    "                                                      \"interrogação\",\"desconfiança\"]))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join([\"crise\",\"crises\",\"recessão\"]))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['focus']))].groupby('data').\\\n",
    "sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['pib','produto interno bruto']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['ibc','ibc br','ibc-br','ibcbr']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join([\"desemprego\",\"desempregado\",\"desempregada\",\"desempregados\",\"desempregadas\",\"emprego\",\"empregos\"]))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['congresso nacional','senado','câmara','pec']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams[df_ngrams['palavra'].str.contains('|'.join(['regulação','norma','resolução','circular']))].\\\n",
    "groupby('data').sum().sort_index().asfreq('D').fillna(0).plot(linewidth = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. INFORMATION RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying text data with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide your own query terms here\n",
    "def query_text(QUERY_TERMS,nr_retornos = 5):\n",
    "\n",
    "    \n",
    "\n",
    "    # Load in human language data from wherever you've saved it\n",
    "    #DATA = 'resources/ch05-textfiles/ch05-timoreilly.json'\n",
    "    #data = json.loads(open(DATA).read())\n",
    "\n",
    "    #activities = [post['content'].lower().split() \n",
    "    #              for post in data \n",
    "    #                if post['content'] != \"\"]\n",
    "\n",
    "    activities = [txt.lower().split() for txt in df['tweet_limpo'] ]\n",
    "    activities_idx = [idx for idx in df['tweet_limpo'].index ]\n",
    "\n",
    "\n",
    "    # TextCollection provides tf, idf, and tf_idf abstractions so\n",
    "    # that we don't have to maintain/compute them ourselves\n",
    "\n",
    "    tc = nltk.TextCollection(activities)\n",
    "\n",
    "    relevant_activities = []\n",
    "\n",
    "    for idx in range(len(activities)):\n",
    "        score = 0\n",
    "        for term in [t.lower() for t in QUERY_TERMS]:\n",
    "            score += tc.tf_idf(term, activities[idx])\n",
    "        if score > 0:\n",
    "            relevant_activities.append({'score': score, 'idx' : activities_idx[idx], 'tweet': activities[idx]})\n",
    "\n",
    "    # Sort by score and display results\n",
    "\n",
    "    relevant_activities = sorted(relevant_activities,\n",
    "                                 key=lambda p: p['score'], reverse=True)\n",
    "    for activity in relevant_activities[:nr_retornos]:\n",
    "        print('Index: {0}'.format(activity['idx']))\n",
    "        print('Tweet: {0}'.format(' '.join(activity['tweet'])))\n",
    "        print('Author: {0}'.format(df[df.index == activity['idx']]['usuario'].values[0]))\n",
    "        print('Data: {0}'.format(df[df.index == activity['idx']]['data'].values[0]))\n",
    "        print('Score: {0}'.format(activity['score']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(QUERY_TERMS = ['covid','coronavirus','coronavírus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(QUERY_TERMS = ['copom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(['política','monetária'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(['autonomia','banco', 'central'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(['reclamar','reclamação','denuncia','denúncia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(['crise','crises','recessão'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text(['boletim','focus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Resultados de análises das críticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Críticas com mais likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.MLP_binary == 'C'][['usuario','tweet','likes']].sort_values(by = 'likes', ascending=False).head(5)['tweet'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Críticas com mais retweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.MLP_binary == 'C'][['usuario','tweet','retweets']].sort_values(by = 'retweets', ascending=False).head(5)['tweet'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usuários mais críticos proporcionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.usuario.value_counts()\n",
    "df_aux = df_aux.to_frame().merge(df[df.MLP_binary == 'C']['usuario'].value_counts(), how='left', left_index=True, right_index=True)\n",
    "df_aux['razao'] = df_aux['usuario_y']/df_aux['usuario_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux[df_aux.usuario_y>10].sort_values(by = 'razao', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario_mais_critico = df_aux[df_aux.usuario_y>10].sort_values(by = 'razao', ascending=False).index[0]\n",
    "usuario_mais_critico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.usuario == usuario_mais_critico)]['tweet'].values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario_mais_critico2 = df_aux[df_aux.usuario_y>10].sort_values(by = 'razao', ascending=False).index[1]\n",
    "usuario_mais_critico2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.usuario == usuario_mais_critico2)]['tweet'].values[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tópicos mais criticados proporcionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.hlda_topics.value_counts()\n",
    "df_aux\n",
    "df_aux = df_aux.to_frame().merge(df[df.MLP_binary == 'C']['hlda_topics'].value_counts(), how='left', left_index=True, right_index=True)\n",
    "df_aux['razao'] = df_aux['hlda_topics_y']/df_aux['hlda_topics_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux[df_aux.hlda_topics_y>10].sort_values(by = 'razao', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_mais_criticado = df_aux[df_aux.hlda_topics_y>10].sort_values(by = 'razao', ascending=False).index[0]\n",
    "mdl.get_topic_words(int(topico_mais_criticado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.hlda_topics == topico_mais_criticado)]['tweet'].values[:5])\n",
    "#df[(df.MLP_binary == 'C') & (df.hlda_topics == topico_mais_criticado)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_mais_criticado2 = df_aux[df_aux.hlda_topics_y>10].sort_values(by = 'razao', ascending=False).index[1]\n",
    "mdl.get_topic_words(int(topico_mais_criticado2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.hlda_topics == topico_mais_criticado2)]['tweet'].values[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunidades mais críticas proporcionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.modularity.value_counts()\n",
    "df_aux\n",
    "df_aux = df_aux.to_frame().merge(df[df.MLP_binary == 'C']['modularity'].value_counts(), how='left', left_index=True, right_index=True)\n",
    "df_aux['razao'] = df_aux['modularity_y']/df_aux['modularity_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux[df_aux.modularity_y>10].sort_values(by = 'razao', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comunidade_mais_critica = df_aux[df_aux.modularity_y>10].sort_values(by = 'razao', ascending=False).index[0]\n",
    "df[df.modularity == comunidade_mais_critica]['usuario'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.modularity == comunidade_mais_critica)]['tweet'].values[:5])\n",
    "#df[(df.MLP_binary == 'C') & (df.modularity == comunidade_mais_critica)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comunidade_mais_critica2 = df_aux[df_aux.modularity_y>10].sort_values(by = 'razao', ascending=False).index[1]\n",
    "df[df.modularity == comunidade_mais_critica2]['usuario'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.MLP_binary == 'C') & (df.modularity == comunidade_mais_critica2)]['tweet'].values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por assuntos específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Críticas à cédula de 200 reais\")\n",
    "df_aux = df[df['MLP_binary']=='C']\n",
    "df_aux[df_aux['tweet'].str.contains('|'.join(['cédula','200 ']))]['tweet'].values[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Críticas ao PIX\")\n",
    "df_aux = df[df['MLP_binary']=='C']\n",
    "df_aux[df_aux['tweet'].str.contains('|'.join(['pix','PIX','instantâneo']))]['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Críticas à liberação do compulsório\")\n",
    "df_aux = df[df['MLP_binary']=='C']\n",
    "df_aux[df_aux['tweet'].str.contains('|'.join(['liberação','compulsório']))]['tweet'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de impacto\n",
    "\n",
    "Copom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_copom = ['(S|s)elic','(C|c)opom','(J|j)uros','SELIC','JUROS','(C|c)omunicado','(A|a)ta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.data>\"2020-03-18 16:00:00+00:00\") & (df.data<\"2020-03-18 20:00:00+00:00\") & (df.tweet.str.contains('|'.join(filtro_copom)))]\\\n",
    "['MLP_binary'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.data>\"2020-05-06 16:00:00+00:00\") & (df.data<\"2020-05-06 20:00:00+00:00\") & (df.tweet.str.contains('|'.join(filtro_copom)))]\\\n",
    "['MLP_binary'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.data>\"2020-06-17 16:00:00+00:00\") & (df.data<\"2020-06-17 20:00:00+00:00\") & (df.tweet.str.contains('|'.join(filtro_copom)))]\\\n",
    "['MLP_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.data>\"2020-08-05 16:00:00+00:00\") & (df.data<\"2020-08-05 20:00:00+00:00\") & (df.tweet.str.contains('|'.join(filtro_copom)))]\\\n",
    "['MLP_binary'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
